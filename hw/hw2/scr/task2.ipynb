{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from itertools import combinations\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 19:35:35 WARN Utils: Your hostname, NotredeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.26.210.20 instead (on interface en0)\n",
      "23/02/23 19:35:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/02/23 19:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# task2.py <filter threshold> <support> <input_file_path> <output_file_path>\n",
    "# t = int(sys.argv[1])\n",
    "# s = int(sys.argv[2])\n",
    "# input_path = sys.argv[3]\n",
    "# output_path = sys.argv[4]\n",
    "# pre_data_path = \"./customer_product.csv\"\n",
    "pre_data_path = \"../data/customer_product.csv\"\n",
    "t = 20\n",
    "s = 50\n",
    "input_path = \"../data/ta_feng_all_months_merged.csv\"\n",
    "output_path = \"../data/output/task2.txt\"\n",
    "\n",
    "s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# pre-process data: \n",
    "    # rename each CUSTOMER_ID as “DATE-CUSTOMER_ID by adding \"TRANSACTION_DT\", eg:“11/14/00-12321”\n",
    "    # header of CSV file should be “DATE-CUSTOMER_ID, PRODUCT_ID”\n",
    "    # need the 1st,2nd and 6th columns\n",
    "    # DATE-CUSTOMER_ID and PRODUCT_ID are strings and integers\n",
    "predata = sc.textFile(input_path)\n",
    "header = predata.take(1)\n",
    "predata = predata.filter(lambda x: x!=header[0])\n",
    "predata = predata.map(lambda x:x.split(\",\")).map(lambda x: [x[0].replace('\"',\"\")+\"-\"+x[1].replace('\"',\"\"),str(int(x[5].replace('\"',\"\")))]).collect()\n",
    "\n",
    "# save pre_data\n",
    "with open(pre_data_path,'w',newline ='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"DATE-CUSTOMER_ID\",\"PRODUCT_ID\"])\n",
    "    for i in predata:\n",
    "        writer.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequent singletons\n",
    "# baskets_l.glom().flatMap(lambda x: get_freq_single(x,4)).distinct().collect()\n",
    "\n",
    "def get_freq_single(baskets,threshold):\n",
    "    counts = {}\n",
    "    freq_singles = []\n",
    "    # count frequency\n",
    "    for basket in baskets:\n",
    "        for singletons in basket:\n",
    "            # if singletons in counts:\n",
    "            #     counts[singletons] += 1\n",
    "            # else:\n",
    "            #     counts[singletons] = 1\n",
    "            counts[singletons] = counts.get(singletons,0)+1\n",
    "    # select candidate that counts over threshold\n",
    "    for candicate in counts.keys():\n",
    "        if counts[candicate] >= threshold:\n",
    "            freq_singles.append(candicate)\n",
    "    return freq_singles\n",
    "\n",
    "# get frequent 1 to k pairs\n",
    "# baskets_l.glom().flatMap(lambda x: get_all_freq_pairs(x,4,3)).distinct().collect()\n",
    "\n",
    "def get_all_freq_pairs(baskets,threshold,max_len):\n",
    "    freq_singles = get_freq_single(baskets,threshold)\n",
    "    if max_len == 1:\n",
    "        return freq_singles\n",
    "    # max_len > 1\n",
    "    prev_freq = freq_singles\n",
    "    all_freq_pairs = [(i,) for i in freq_singles]\n",
    "    pair_size = 2\n",
    "    while pair_size <= max_len:\n",
    "        counts = {}\n",
    "        freq_k_pair = []\n",
    "        if pair_size == 2:\n",
    "            accu_prev_freq = freq_singles\n",
    "        else:\n",
    "            accu_prev_freq = set()\n",
    "            for pair in prev_freq:\n",
    "                for item in pair:\n",
    "                    accu_prev_freq.add(item)\n",
    "            # accu_prev_freq = sorted(accu_prev_freq)\n",
    "\n",
    "        # construct pari of size k by previous frequent items/pairs\n",
    "        # k_pair = combinations(accu_prev_freq,pair_size)\n",
    "        # # count frequency\n",
    "        # for pair in k_pair:\n",
    "        #     for basket in baskets:\n",
    "        #         # check if pair in single basket\n",
    "        #         if all(x in basket for x in pair):\n",
    "        #             counts[pair] = counts.get(pair,0)+1\n",
    "        # or\n",
    "        for basket in baskets:\n",
    "            basket = sorted(set(basket).intersection(set(accu_prev_freq)))\n",
    "            k_pair = combinations(basket,pair_size)\n",
    "            for pair in k_pair:\n",
    "                # pair = tuple(pair)\n",
    "                counts[pair] = counts.get(pair,0)+1\n",
    "\n",
    "        # select candidate pair that counts over threshold\n",
    "        for candicate in counts.keys():\n",
    "            if counts[candicate] >= threshold:\n",
    "                freq_k_pair.append(candicate)\n",
    "        # remove duplicates\n",
    "        freq_k_pair = [tuple(sorted(i)) for i in freq_k_pair]\n",
    "        freq_k_pair = sorted(list(set(freq_k_pair)))\n",
    "        all_freq_pairs += freq_k_pair\n",
    "        # update next pari size\n",
    "        pair_size += 1\n",
    "        prev_freq = freq_k_pair\n",
    "    return all_freq_pairs\n",
    "\n",
    "# implment son alg: find n(item)>new_threshold\n",
    "def son_alg(baskets,baskets_size,support):\n",
    "    max_len = max([len(i) for i in baskets])\n",
    "    new_threshold = math.ceil(len(baskets)/baskets_size*support)\n",
    "    all_freq_pairs = get_all_freq_pairs(baskets,new_threshold,max_len)\n",
    "    return all_freq_pairs\n",
    "\n",
    "# counts candidate frequent in total\n",
    "def counts_in_total(baskets,candidates):\n",
    "    counts = {}\n",
    "    for item in candidates:\n",
    "        for basket in baskets:\n",
    "            if all(x in basket for x in item):\n",
    "                counts[item] = counts.get(item,0)+1\n",
    "    res = [(i,counts[i]) for i in counts.keys()]\n",
    "    return res\n",
    "\n",
    "def output_format(res):\n",
    "    # output = {i+1:[] for i in range(max(len(i) for i in inter_res))}\n",
    "    output = {}\n",
    "    for i in res:\n",
    "        new_item = \"('\"+\"', '\".join(list(i))+\"'),\"\n",
    "        if len(i) in output:\n",
    "            output[len(i)] = output[len(i)]+new_item\n",
    "        else:\n",
    "            output[len(i)] = new_item\n",
    "    # max_len = max(len(i) for i in res)\n",
    "    output_txt = \"\"\n",
    "    for i in output.keys():\n",
    "        # if i != max_len:\n",
    "        output_txt += output[i][:-1]+\"\\n\\n\"\n",
    "    return output_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predata\n",
    "data = sc.textFile(pre_data_path)\n",
    "data = data.filter(lambda x: x!=\"DATE-CUSTOMER_ID,PRODUCT_ID\") #exclude the first line of name\n",
    "data = data.map(lambda x: (x.split(\",\")[0],[x.split(\",\")[1]])).reduceByKey(lambda x,y: x+y)\n",
    "# remove the dupplicates\n",
    "data = data.mapValues(lambda x: [*set(x)])\n",
    "# Find out qualified customers-date who purchased more than t items\n",
    "data = data.filter(lambda x: len(x[1])>t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# implement son_alg on filtered market-basket model\n",
    "# pass 1: get candidate frequent itemset\n",
    "baskets_l = data.map(lambda x: x[1])\n",
    "baskets_size = baskets_l.count()\n",
    "all_freq = baskets_l.glom().flatMap(lambda x: son_alg(x,baskets_size,s)).distinct().collect()\n",
    "inter_res = sorted(all_freq, key=lambda x: [len(x), x])\n",
    "# all_freq = baskets_l.glom().flatMap(lambda x: son_alg(x, baskets_size, s)).distinct()\n",
    "# inter_res = all_freq.sortBy(lambda x: (len(x), x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# pass 2: counts candidates and select true frequent\n",
    "true_freq = baskets_l.glom().flatMap(lambda x: counts_in_total(x,inter_res)).reduceByKey(lambda x,y: x+y).\\\n",
    "    filter(lambda x: x[1]>=s).map(lambda x: x[0]).collect()\n",
    "final_res = sorted(true_freq, key=lambda x: [len(x), x])\n",
    "# true_freq = baskets_l.glom().flatMap(lambda x: counts_in_total(x,inter_res)).reduceByKey(lambda x,y: x+y).\\\n",
    "#     filter(lambda x: x[1]>=s).map(lambda x: x[0]).distinct()\n",
    "# final_res = true_freq.sortBy(lambda x: (len(x), x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path,\"w\") as f:\n",
    "    output_txt = \"Candidates:\\n\"+output_format(inter_res)+\"Frequent Itemsets:\\n\"+output_format(final_res)\n",
    "    f.write(output_txt[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 10.889840841293335\n"
     ]
    }
   ],
   "source": [
    "e_time = time.time()\n",
    "duration = e_time-s_time\n",
    "print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "#/opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --executor-memory 4G --driver-memory 4G task2.py\n",
    "# 20 50\n",
    "# \"../resource/asnlib/publicdata/ta_feng_all_months_merged.csv\"\n",
    "# \"./task2.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
