{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from itertools import combinations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1.py <case number> <support> <input_file_path> <output_file_path>\n",
    "# case_num = int(sys.argv[1])\n",
    "# s = int(sys.argv[2])\n",
    "# input_path = sys.argv[3]\n",
    "# output_path = sys.argv[4]\n",
    "case_num = 1\n",
    "s = 4\n",
    "input_path = \"../data/small1_test.csv\"\n",
    "output_path = \"../data/output/task1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/19 00:35:56 WARN Utils: Your hostname, NotredeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.168 instead (on interface en0)\n",
      "23/02/19 00:35:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/02/19 00:35:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task1\").getOrCreate()\n",
    "data = sc.textFile(input_path).filter(lambda x: x!=\"user_id,business_id\") #exclude the first line of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if case_num == 1:\n",
    "    # merge bysiness id\n",
    "    data = data.map(lambda x: (x.split(\",\")[0],[x.split(\",\")[1]])).reduceByKey(lambda x,y: x+y)\n",
    "else:\n",
    "    data = data.map(lambda x: (x.split(\",\")[1],[x.split(\",\")[0]])).reduceByKey(lambda x,y: x+y)\n",
    "# remove the dupplicates\n",
    "data = data.mapValues(lambda x: [*set(x)])\n",
    "baskets_l = data.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequent singletons\n",
    "# baskets_l.glom().flatMap(lambda x: get_freq_single(x,4)).distinct().collect()\n",
    "\n",
    "def get_freq_single(baskets,threshold):\n",
    "    counts = {}\n",
    "    freq_singles = []\n",
    "    # count frequency\n",
    "    for basket in baskets:\n",
    "        for singletons in basket:\n",
    "            # if singletons in counts:\n",
    "            #     counts[singletons] += 1\n",
    "            # else:\n",
    "            #     counts[singletons] = 1\n",
    "            counts[singletons] = counts.get(singletons,0)+1\n",
    "    # select candidate that counts over threshold\n",
    "    for candicate in counts.keys():\n",
    "        if counts[candicate] >= threshold:\n",
    "            freq_singles.append(candicate)\n",
    "    return freq_singles\n",
    "\n",
    "# get frequent 1 to k pairs\n",
    "# baskets_l.glom().flatMap(lambda x: get_all_freq_pairs(x,4,3)).distinct().collect()\n",
    "\n",
    "def get_all_freq_pairs(baskets,threshold,max_len):\n",
    "    freq_singles = get_freq_single(baskets,threshold)\n",
    "    if max_len == 1:\n",
    "        return freq_singles\n",
    "    # max_len > 1\n",
    "    prev_freq = freq_singles\n",
    "    all_freq_pairs = [(i,) for i in freq_singles]\n",
    "    pair_size = 2\n",
    "    while pair_size <= max_len:\n",
    "        counts = {}\n",
    "        freq_k_pair = []\n",
    "        if pair_size == 2:\n",
    "            accu_prev_freq = freq_singles\n",
    "        else:\n",
    "            accu_prev_freq = set()\n",
    "            for pair in prev_freq:\n",
    "                for item in pair:\n",
    "                    accu_prev_freq.add(item)\n",
    "            # accu_prev_freq = sorted(accu_prev_freq)\n",
    "        # construct pari of size k by previous frequent items/pairs\n",
    "        k_pair = combinations(accu_prev_freq,pair_size)\n",
    "        # count frequency\n",
    "        for pair in k_pair:\n",
    "            for basket in baskets:\n",
    "                # check if pair in single basket\n",
    "                if all(x in basket for x in pair):\n",
    "                    counts[pair] = counts.get(pair,0)+1\n",
    "        # select candidate pair that counts over threshold\n",
    "        for candicate in counts.keys():\n",
    "            if counts[candicate] >= threshold:\n",
    "                freq_k_pair.append(candicate)\n",
    "        # remove duplicates\n",
    "        freq_k_pair = [tuple(sorted(i)) for i in freq_k_pair]\n",
    "        freq_k_pair = sorted(list(set(freq_k_pair)))\n",
    "        all_freq_pairs += freq_k_pair\n",
    "        # update next pari size\n",
    "        pair_size += 1\n",
    "        prev_freq = freq_k_pair\n",
    "    return all_freq_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implment son alg: find n(item)>new_threshold\n",
    "def son_alg(baskets,baskets_size,support):\n",
    "    max_len = max([len(i) for i in baskets])\n",
    "    new_threshold = math.ceil(len(baskets)/baskets_size*support)\n",
    "    all_freq_pairs = get_all_freq_pairs(baskets,new_threshold,max_len)\n",
    "    return all_freq_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts candidate frequent in total\n",
    "def counts_in_total(baskets,candidates):\n",
    "    counts = {}\n",
    "    for item in candidates:\n",
    "        for basket in baskets:\n",
    "            if all(x in basket for x in item):\n",
    "                counts[item] = counts.get(item,0)+1\n",
    "    res = [(i,counts[i]) for i in counts.keys()]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass 1: get candidate frequent itemset\n",
    "baskets_size = baskets_l.count()\n",
    "all_freq = baskets_l.glom().flatMap(lambda x: son_alg(x,baskets_size,s)).distinct().collect()\n",
    "inter_res = sorted(all_freq, key=lambda x: [len(x), x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass 2: counts candidates and select true frequent\n",
    "true_freq = baskets_l.glom().flatMap(lambda x: counts_in_total(x,inter_res)).reduceByKey(lambda x,y: x+y).\\\n",
    "    filter(lambda x: x[1]>=s).map(lambda x: x[0]).collect()\n",
    "final_res = sorted(true_freq, key=lambda x: [len(x), x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_format(res):\n",
    "    # output = {i+1:[] for i in range(max(len(i) for i in inter_res))}\n",
    "    output = {}\n",
    "    for i in res:\n",
    "        new_item = \"('\"+\"', '\".join(list(i))+\"'),\"\n",
    "        if len(i) in output:\n",
    "            output[len(i)] = output[len(i)]+new_item\n",
    "        else:\n",
    "            output[len(i)] = new_item\n",
    "    # max_len = max(len(i) for i in res)\n",
    "    output_txt = \"\"\n",
    "    for i in output.keys():\n",
    "        # if i != max_len:\n",
    "        output_txt += output[i][:-1]+\"\\n\\n\"\n",
    "    return output_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path,\"w\") as f:\n",
    "    output_txt = \"Candidate:\\n\"+output_format(inter_res)+\"Frequent Itemsets:\\n\"+output_format(final_res)\n",
    "    f.write(output_txt[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1676795755.336503\n"
     ]
    }
   ],
   "source": [
    "e_time = time.time()\n",
    "duration = e_time=s_time\n",
    "print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "#/opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --executor-memory 4G --driver-memory 4G task1.py\n",
    "# 1 2\n",
    "# 4 9\n",
    "# \"../resource/asnlib/publicdata/small1.csv\"\n",
    "# \"./task1.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
