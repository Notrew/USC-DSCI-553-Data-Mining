{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methood description:\n",
    "\n",
    "# on validation set\n",
    "# Error Distribution:\n",
    "# >=0 and <1: \n",
    "# >=1 and <2: \n",
    "# >=2 and <3: \n",
    "# >=3 and <4: \n",
    "# >=4: 12\n",
    "\n",
    "# RMSE:\n",
    "\n",
    "# Execution time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition.py <folder_path> <test_file_path> <output_file_path>\n",
    "# folder_path = sys.argv[1]\n",
    "# test_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "folder_path = \"../data/input/\"\n",
    "test_path = \"../data/input/yelp_val.csv\"\n",
    "output_path = \"../data/output/task2_2.csv\"\n",
    "\n",
    "train_path = folder_path+\"yelp_train.csv\"\n",
    "user_path = folder_path+\"user.json\"\n",
    "business_path = folder_path+\"business.json\"\n",
    "review_train_path = folder_path+\"review_train.json\"\n",
    "photo_path = folder_path+\"photo.json\"\n",
    "checkin_path = folder_path+\"checkin.json\"\n",
    "tip_path = folder_path+\"tip.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/05/01 23:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local[*]\",appName=\"competition\").getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPriceRange(attributes,key):\n",
    "    if attributes:\n",
    "        if key in attributes.keys():\n",
    "            return float(attributes.get(key))\n",
    "    return 0\n",
    "\n",
    "def convertBi(attributes,key):\n",
    "    if attributes and key in attributes.keys():\n",
    "        if attributes[key] != \"False\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# var_rate and photo_cnt may be None\n",
    "def fillInNone(num,default):\n",
    "    if num:\n",
    "        return num\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "# get and join features together, all inputs are dictionaries\n",
    "def mergrFeatures(df_org,uid_info,bid_info):\n",
    "    col_names = [\"user_review_cnt\",\"user_fans\",\"user_avg_rate\",\"user_var_rate\",\"user_friends\",\"user_social\",\"user_year\",\"user_elite\",\"user_compliment\",\"user_tip\",\n",
    "                 \"bsn_avg_rate\",\"bsn_var_rate\",\"bsn_review_cnt\",\"bsn_price_range\",\"bsn_photo_cnt\",\"bsn_attribute_score\",\"bsn_checkin\",\"bsn_tip\"]\n",
    "    user_review_cnt = []\n",
    "    user_fans = []\n",
    "    user_avg_rate = []\n",
    "    user_var_rate = []\n",
    "    user_friends = []\n",
    "    user_social = []\n",
    "    user_year = []\n",
    "    user_elite = []\n",
    "    user_compliment = []\n",
    "    user_tip = []\n",
    "    \n",
    "    bsn_avg_rate = []\n",
    "    bsn_var_rate = []\n",
    "    bsn_review_cnt = []\n",
    "    bsn_price_range = [ ]\n",
    "    bsn_photo_cnt = []\n",
    "    bsn_attribute_score = []\n",
    "    bsn_checkin = []\n",
    "    bsn_tip = []\n",
    "\n",
    "    # uid_info: {user_id:(review_count,fans,average_stars,friends,social,year,elite,compliment,var_rate,tips)}\n",
    "    for uid in df_org[\"user_id\"]:\n",
    "        if uid in uid_info.keys():\n",
    "            user_review_cnt.append(uid_info.get(uid)[0])\n",
    "            user_fans.append(uid_info.get(uid)[1])\n",
    "            user_avg_rate.append(uid_info.get(uid)[2])\n",
    "            user_friends.append(uid_info.get(uid)[3])\n",
    "            user_social.append(uid_info.get(uid)[4])\n",
    "            user_year.append(uid_info.get(uid)[5])\n",
    "            user_elite.append(uid_info.get(uid)[6])\n",
    "            user_compliment.append(uid_info.get(uid)[7])\n",
    "            user_var_rate.append(uid_info.get(uid)[8])\n",
    "            user_tip.append(uid_info.get(uid)[9])\n",
    "        else:\n",
    "            user_review_cnt.append(uid_review_cnt_whole)\n",
    "            user_fans.append(uid_fans_whole)\n",
    "            user_avg_rate.append(uid_avg_rate_whole)\n",
    "            user_friends.append(uid_fri_whole)\n",
    "            user_social.append(uid_social_whole)\n",
    "            user_year.append(0)\n",
    "            user_elite.append(0)\n",
    "            user_compliment.append(0)\n",
    "            user_var_rate.append(0)\n",
    "            user_tip.append(0)\n",
    "    # bid_info: {business_id:(stars,review_count,price_range,accribute_score,var_rate,phtot_cnt,checkin_time,tip)}\n",
    "    for bid in df_org[\"business_id\"]:\n",
    "        if bid in bid_info.keys():\n",
    "            bsn_avg_rate.append(bid_info.get(bid)[0])\n",
    "            bsn_review_cnt.append(bid_info.get(bid)[1])\n",
    "            bsn_price_range.append(bid_info.get(bid)[2])\n",
    "            bsn_attribute_score.append(bid_info.get(bid)[3])\n",
    "            bsn_var_rate.append(bid_info.get(bid)[4])\n",
    "            bsn_photo_cnt.append(bid_info.get(bid)[5])\n",
    "            bsn_checkin.append(bid_info.get(bid)[6])\n",
    "            bsn_tip.append(bid_info.get(bid)[7])\n",
    "\n",
    "        else:\n",
    "            bsn_avg_rate.append(bid_avg_rate_whole)\n",
    "            # bsn_review_cnt.append(bid_review_cnt_whole)\n",
    "            # bsn_price_range.append(bid_price_range_whole)\n",
    "            bsn_attribute_score.append(0)\n",
    "            bsn_var_rate.append(0)\n",
    "            bsn_photo_cnt.append(0)\n",
    "            bsn_checkin.append(0)\n",
    "            bsn_tip.append(0)\n",
    "            # bsn_avg_rate.append(3)\n",
    "            bsn_review_cnt.append(0)\n",
    "            bsn_price_range.append(0)\n",
    "    for i in col_names:\n",
    "        df_org[i] = locals()[i]\n",
    "    return df_org\n",
    "\n",
    "def assignError(pred,truth,length):\n",
    "    from_0_to_1 = 0\n",
    "    from_1_to_2 = 0\n",
    "    from_2_to_3 = 0\n",
    "    from_3_to_4 = 0\n",
    "    from_4_to_5 = 0\n",
    "    for i in range(length):\n",
    "        diff = abs(truth[i]-pred[i])\n",
    "        if diff >= 0 and diff < 1:\n",
    "            from_0_to_1 += 1\n",
    "        elif diff >= 1 and diff < 2:\n",
    "            from_1_to_2 += 1\n",
    "        elif diff >= 2 and diff < 3:\n",
    "            from_2_to_3 += 1\n",
    "        elif diff >= 3 and diff < 4:\n",
    "            from_3_to_4 += 1\n",
    "        elif diff >= 4:\n",
    "            from_4_to_5 += 1\n",
    "    return [from_0_to_1,from_1_to_2,from_2_to_3,from_3_to_4,from_4_to_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre_datasets, select features\n",
    "# train_data\n",
    "train_data = sc.textFile(train_path)\n",
    "head = train_data.first()\n",
    "train_data = train_data.filter(lambda x: x!=head) #exclude the first line of name\n",
    "train_uid_bid_rate = train_data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1],float(x[2])))\n",
    "\n",
    "# user, select: user_id,(review_count,fans,average_stars,friends,useful+funny+cool,year,elite,compliment))\n",
    "user = sc.textFile(user_path).map(lambda x: json.loads(x))\n",
    "user = user.map(lambda x: (x[\"user_id\"],(x[\"review_count\"],x[\"fans\"],x[\"average_stars\"],len(x[\"friends\"].split(\",\")),x[\"useful\"]+x[\"funny\"]+x[\"cool\"],\n",
    "                                         (2023-int(x[\"yelping_since\"][:4])),len(x[\"elite\"].split(\",\")),x[\"compliment_hot\"]+x[\"compliment_more\"]+\\\n",
    "                                         x[\"compliment_profile\"]+x[\"compliment_cute\"]+x[\"compliment_list\"]+x[\"compliment_note\"]+x[\"compliment_plain\"]+\\\n",
    "                                         x[\"compliment_cool\"]+x[\"compliment_funny\"]+x[\"compliment_writer\"]+x[\"compliment_photos\"]\n",
    "                                         )))\n",
    "\n",
    "# business, select: business_id,(stars,review_count,priceRange,attribute_score)) \n",
    "business = sc.textFile(business_path).map(lambda x: json.loads(x))\n",
    "business = business.map(lambda x: (x[\"business_id\"],(x[\"stars\"],x[\"review_count\"],getPriceRange(x[\"attributes\"],\"RestaurantsPriceRange2\"),\n",
    "                                                    convertBi(x[\"attributes\"],\"OutdoorSeating\")+convertBi(x[\"attributes\"],\"RestaurantsDelivery\")+\\\n",
    "                                                    convertBi(x[\"attributes\"],\"RestaurantsGoodForGroups\")+convertBi(x[\"attributes\"],\"RestaurantsReservations\")+\\\n",
    "                                                    convertBi(x[\"attributes\"],\"RestaurantsTakeOut\")+convertBi(x[\"attributes\"],\"BikeParking\")+\\\n",
    "                                                    convertBi(x[\"attributes\"],\"BusinessAcceptsCreditCards\")+convertBi(x[\"attributes\"],\"GoodForKids\")+\\\n",
    "                                                    convertBi(x[\"attributes\"],\"HasTV\")\n",
    "                                                        )))\n",
    "\n",
    "# review_train, (user_id,business_id,stars)\n",
    "review_train = sc.textFile(review_train_path).map(lambda x: json.loads(x))\n",
    "review_train = review_train.map(lambda x: (x[\"user_id\"],x[\"business_id\"],x[\"stars\"]))\n",
    "\n",
    "# photo, select:business_id,label(['food', 'drink', 'outside', 'inside', 'menu'])\n",
    "photo = sc.textFile(photo_path).map(lambda x: json.loads(x))\n",
    "photo = photo.map(lambda x: (x[\"business_id\"],x[\"label\"]))\n",
    "\n",
    "# check_in, (business_id,checkin_time)\n",
    "checkin = sc.textFile(checkin_path).map(lambda x: json.loads(x))\n",
    "checkin = checkin.map(lambda x: (x[\"business_id\"],len(x[\"time\"])))\n",
    "\n",
    "# tip, (user_id,business_id,likes)\n",
    "tip = sc.textFile(tip_path).map(lambda x: json.loads(x))\n",
    "tip = tip.map(lambda x: (x[\"user_id\"],x[\"business_id\"],x[\"likes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# aggragation\n",
    "# user, select: user_id,(review_count,fans,average_stars,friends,useful+funny+cool,year,elite,compliment))\n",
    "# review_cnt\n",
    "# if uid not in extra dataset, use the average review_cnt in extra dataset \n",
    "uid_review_cnt_whole = user.map(lambda x: x[1][0]).mean()\n",
    "# fans\n",
    "uid_fans_whole = user.map(lambda x: x[1][1]).mean()\n",
    "# avg_rate\n",
    "uid_avg_rate_whole = user.map(lambda x: x[1][2]).mean()\n",
    "# friends\n",
    "uid_fri_whole = user.map(lambda x: x[1][3]).mean()\n",
    "# (usrful+funny+cool)\n",
    "uid_social_whole = user.map(lambda x: x[1][4]).mean()\n",
    "# var_rate\n",
    "uid_var_rate = review_train.map(lambda x: (x[0],x[2])).groupByKey().mapValues(lambda x: np.var((list(x))))\n",
    "# tip\n",
    "uid_tips = tip.map(lambda x: (x[0],x[2])).groupByKey().mapValues(lambda x: sum(list(x)))\n",
    "# uid_info: {user_id:(review_count,fans,average_stars,friends,social,year,elite,compliment,var_rate,tips)}\n",
    "uid_info = user.leftOuterJoin(uid_var_rate).map(lambda x: x).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).\\\n",
    "    leftOuterJoin(uid_tips).map(lambda x: x).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# business, select: business_id,(stars,review_count,priceRange,accribute_score)) \n",
    "\n",
    "# avg_rate\n",
    "bid_avg_rate_whole = business.map(lambda x: x[1][0]).mean()\n",
    "# var_rate\n",
    "bid_var_rate = review_train.map(lambda x: (x[1],x[2])).groupByKey().mapValues(lambda x: np.var((list(x))))\n",
    "# review_cnt\n",
    "bid_review_cnt_whole = business.map(lambda x: x[1][1]).mean()\n",
    "# price_range\n",
    "bid_price_range_whole = business.map(lambda x: x[1][2]).mean()\n",
    "# photo_cnt\n",
    "bid_photo_cnt = photo.filter(lambda x: x[1]!=\"menu\").map(lambda x: (x[0],1)).reduceByKey(lambda x,y:x+y)\n",
    "# tip\n",
    "bid_tips = tip.map(lambda x: (x[1],x[2])).groupByKey().mapValues(lambda x: sum(list(x)))\n",
    "# bid_info: {business_id:(stars,review_count,price_range,accribute_score,var_rate,phtot_cnt,checkin_time,tip)}\n",
    "bid_info = business.leftOuterJoin(bid_var_rate).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).\\\n",
    "                    leftOuterJoin(bid_photo_cnt).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).\\\n",
    "                    leftOuterJoin(checkin).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).\\\n",
    "                    leftOuterJoin(bid_tips).map(lambda x: x).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train_org = pd.DataFrame(train_uid_bid_rate.collect(),columns=[\"user_id\",\"business_id\",\"stars\"])\n",
    "df_train = mergrFeatures(df_train_org,uid_info,bid_info)\n",
    "x_train = df_train.drop([\"user_id\",\"business_id\",\"stars\"],axis=1)\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# standarize\n",
    "# for col in x_train.columns:\n",
    "#     x_train[col] = (x_train[col]-x_train[col].mean())/x_train[col].std()\n",
    "y_train = df_train[\"stars\"]\n",
    "\n",
    "# read test data and train and get basic info\n",
    "# s_time = time.time()\n",
    "test_data = sc.textFile(test_path)\n",
    "test_head = test_data.first()\n",
    "test_data = test_data.filter(lambda x: x!=test_head) #exclude the first line of name\n",
    "uid_bid_to_pred = test_data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1])).collect()\n",
    "df_test_org = pd.DataFrame(uid_bid_to_pred,columns=[\"user_id\",\"business_id\"])\n",
    "df_test = mergrFeatures(df_test_org,uid_info,bid_info)\n",
    "x_test = df_test.drop([\"user_id\",\"business_id\"],axis=1)\n",
    "# x_test = scaler.transform(x_test)\n",
    "# for col in x_test.columns:\n",
    "#     x_test[col] = (x_test[col]-x_test[col].mean())/x_test[col].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/z_2frwj97bxfjbt6808sw4_00000gn/T/ipykernel_15488/1324971932.py:1: FutureWarning: The default value of numeric_only in DataFrame.std is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_train.std().sort_values()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_avg_rate              0.382592\n",
       "bsn_var_rate               0.486690\n",
       "user_var_rate              0.560533\n",
       "bsn_avg_rate               0.586569\n",
       "bsn_price_range            0.781764\n",
       "stars                      1.126781\n",
       "bsn_accribute_score        2.204562\n",
       "user_year                  2.427212\n",
       "user_elite                 2.587253\n",
       "bsn_tip                    4.205548\n",
       "user_tip                  10.858236\n",
       "bsn_checkin               37.133507\n",
       "bsn_photo_cnt             62.125949\n",
       "user_fans                102.415886\n",
       "user_review_cnt          631.332997\n",
       "user_friends             668.094078\n",
       "bsn_review_cnt           743.234965\n",
       "user_compliment         3121.067154\n",
       "user_social            13962.921597\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train.std().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0.5, 0.7) 0.980911756275494\n",
      "(6, 0.5, 0.8) 0.9808813345444787\n",
      "(6, 0.7, 0.6) 0.9807839885045408\n",
      "(6, 0.8, 0.5) 0.9807333566512458\n",
      "(7, 0.5, 0.6) 0.9806775849146323\n",
      "(7, 0.5, 0.7) 0.9805781453563329\n",
      "(7, 0.6, 0.6) 0.9804242492179991\n",
      "(7, 0.7, 0.6) 0.9803326290449933\n",
      "(7, 0.8, 0.5) 0.9802348477166134\n",
      "(8, 0.7, 0.5) 0.9801929966106603\n",
      "(8, 0.7, 0.7) 0.9801825299906429\n",
      "(8, 0.8, 0.5) 0.9798141658557629\n"
     ]
    }
   ],
   "source": [
    "# # select parameters\n",
    "# y_true = test_data.map(lambda x: x.split(\",\")).map(lambda x: float(x[2])).collect()\n",
    "# res = 0.981\n",
    "# for alpha in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "#     for lr in [0.05,0.01,0.1]:\n",
    "# for max_depth in [3,4,5,6,7,8]:\n",
    "#     for subsample in [0.5,0.6,0.7,0.8]:\n",
    "#         for colsample_bytree in [0.5,0.6,0.7,0.8]:\n",
    "#             xgb_model = xgb.XGBRegressor(alpha=0.8,learning_rate=0.1,max_depth=max_depth,subsample=subsample,colsample_bytree=colsample_bytree,random_state=0)\n",
    "#             xgb_model.fit(x_train.drop([\"bsn_var_rate\",\"user_var_rate\",\"bsn_price_range\"],axis=1),y_train)\n",
    "#             y_pred = xgb_model.predict(x_test.drop([\"bsn_var_rate\",\"user_var_rate\",\"bsn_price_range\"],axis=1))\n",
    "#             rmse = mean_squared_error(np.array(y_true),y_pred,squared=False)\n",
    "#             if rmse <= res:\n",
    "#                 res = rmse\n",
    "#                 print((max_depth,subsample,colsample_bytree),res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "# xgb_model = xgb.XGBRegressor(alpha=0.8,learning_rate=0.1,colsample_bytree=0.4,max_depth=7,n_estimators=200,subsample=0.6,random_state=0)\n",
    "xgb_model = xgb.XGBRegressor(alpha=0.8,learning_rate=0.1,colsample_bytree=0.4,max_depth=8,n_estimators=200,subsample=0.8,random_state=0)\n",
    "\n",
    "xgb_model.fit(x_train.drop([\"bsn_var_rate\",\"user_var_rate\",\"bsn_price_range\"],axis=1),y_train)\n",
    "# xgb_model.fit(x_train,y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = xgb_model.predict(x_test.drop([\"bsn_var_rate\",\"user_var_rate\",\"bsn_price_range\"],axis=1))\n",
    "# y_pred = xgb_model.predict(x_test)\n",
    "output = pd.DataFrame({\"user_id\":[x[0] for x in uid_bid_to_pred],\"business_id\":[x[1] for x in uid_bid_to_pred],\"prediction\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979435927635686\n",
      "[102203, 32853, 6165, 821, 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = test_data.map(lambda x: x.split(\",\")).map(lambda x: float(x[2])).collect()\n",
    "# calculate RMSE < 0.98\n",
    "print(math.sqrt(mean_squared_error(np.array(y_true),y_pred)))\n",
    "print(assignError(y_pred,np.array(y_true),len(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "# rfecv = RFECV(estimator=xgb_model,step=1,scoring='neg_root_mean_squared_error')\n",
    "# rfecv.fit(x_train,y_train)\n",
    "# import matplotlib.pyplot as plt\n",
    "# print('Optimal number of features :', rfecv.n_features_)\n",
    "# print('Best features :', x_train.columns[rfecv.support_])\n",
    "# print('Original features :', x_train.columns)\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score \\n of number of selected features\")\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 100 second\n",
    "# e_time = time.time()\n",
    "# duration = e_time-s_time\n",
    "# print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv, header: user_id, business_id, prediction\n",
    "# output.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/02 01:03:43 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3080465 ms exceeds timeout 120000 ms\n",
      "23/05/02 01:03:43 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Methood description:\n",
    "\n",
    "# on validation set\n",
    "# Error Distribution:\n",
    "# >=0 and <1: \n",
    "# >=1 and <2: \n",
    "# >=2 and <3: \n",
    "# >=3 and <4: \n",
    "# >=4: \n",
    "\n",
    "# RMSE:\n",
    "\n",
    "# Execution time:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
