{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from pyspark import SparkContext\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFR\n",
    "# task.py <input_path> <n_cluster> <output_path>\n",
    "# input_path = sys.argv[1]\n",
    "# n_cluster = int(sys.argv[2])\n",
    "# output_path = sys.argv[3]\n",
    "input_path = \"../data/input/hw6_clustering.txt\"\n",
    "n_cluster = 50\n",
    "output_path = \"../data/output/task.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/04/21 21:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task\").getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = sc.textFile(input_path)\n",
    "# data point index,cluster index,features of data point\n",
    "# [index,[features,...]]\n",
    "data = data.map(lambda x: (int(x.split(\",\")[0]),[float(feature) for feature in x.split(\",\")[2:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The initialization of DS\n",
    "DS = {}\n",
    "CS = {}\n",
    "RS = {}\n",
    "\n",
    "# Step 1. Load 20% of the data randomly.\n",
    "random.seed(553)\n",
    "a = random.randint(1,round(time.time()/10000))\n",
    "b = random.randint(1,round(time.time()/10000))\n",
    "def hash_to_5_part(index):\n",
    "    hash_value = ((a*index+b)%2023)%5\n",
    "    return int(hash_value)\n",
    "\n",
    "data = data.map(lambda x: (x[0],hash_to_5_part(x[0]),x[1]))\n",
    "# random_num = random.randint(0,4)\n",
    "# print(random_num)\n",
    "random_num = 0\n",
    "# {index,[features,...]}\n",
    "sample = data.filter(lambda x: x[1]==random_num).map(lambda x: (x[0],x[2])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Run K-Means with a large K (e.g.,5 times of the number of the input clusters) on the data in memory \n",
    "# using the Euclidean distance as the similarity measurement.\n",
    "kmeans = KMeans(n_clusters=n_cluster*5,random_state=0).fit(list(sample.values()))\n",
    "# kmeans = KMeans(n_clusters=n_cluster*5).fit(list(sample.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. \n",
    "# count labels \n",
    "labels = kmeans.labels_\n",
    "# {index,cluster}\n",
    "label_info = {}\n",
    "# {cluster,cnt}\n",
    "label_cnt = {}\n",
    "for i in range(len(sample.keys())):\n",
    "    label_info[list(sample.keys())[i]] = labels[i]\n",
    "    if labels[i] not in label_cnt:\n",
    "        label_cnt[labels[i]] = 1\n",
    "    else:\n",
    "        label_cnt[labels[i]] += 1\n",
    "\n",
    "# move all the clusters that contain only one point to RS (outliers)\n",
    "cluster_cnt_1 = []\n",
    "for label in label_cnt:\n",
    "    if label_cnt[label] == 1:\n",
    "        cluster_cnt_1.append(label)\n",
    "\n",
    "sample_2 = copy.deepcopy(sample)\n",
    "for index in label_info:\n",
    "    if label_info[index] in cluster_cnt_1:\n",
    "        RS[index] = sample[index]\n",
    "        sample_2.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Run K-Means again to cluster the rest of the data points with K = the number of input clusters.\n",
    "# kmeans_2 = KMeans(n_clusters=n_cluster,random_state=0).fit(list(sample_2.values()))\n",
    "kmeans_2 = KMeans(n_clusters=n_cluster).fit(list(sample_2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_DS_RS(sample,labels,current_ds,current_rs):\n",
    "    # {cluster_index,data_index}\n",
    "    cluster_info = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in cluster_info:\n",
    "            cluster_info[labels[i]] = [list(sample.keys())[i]]\n",
    "        else:\n",
    "            cluster_info[labels[i]].append(list(sample.keys())[i])\n",
    "        \n",
    "    # gemerate ds and rs\n",
    "    for cluster in cluster_info:\n",
    "        members = cluster_info[cluster]\n",
    "        # if contain only one point\n",
    "        if len(members) <= 1:\n",
    "            current_rs[members[0]] = sample[members[0]]\n",
    "        else:\n",
    "            features = [sample[i] for i in members]\n",
    "            tmp = np.array(features)\n",
    "            # generate statistics\n",
    "            cluster_n = len(tmp)\n",
    "            cluster_sum = tmp.sum(axis=0)\n",
    "            cluster_sumsq = (tmp**2).sum(axis=0)\n",
    "            current_ds[cluster] = [members,cluster_n,cluster_sum,cluster_sumsq]\n",
    "    \n",
    "    return current_ds,current_rs\n",
    "\n",
    "def generate_CS_RS(current_cs,current_rs):\n",
    "    # current_cs = copy.deepcopy(current_cs)\n",
    "    # current_rs = copy.deepcopy(current_rs)\n",
    "    # if # of points in RS more than # of 5*n_cluster\n",
    "    if len(current_rs) > 5*n_cluster:\n",
    "        tmp = list(current_rs.values())\n",
    "        model = KMeans(n_clusters=5*n_cluster,random_state=0).fit(tmp)\n",
    "        labels = model.labels_\n",
    "        current_cs,current_rs = generate_DS_RS(current_rs,labels,current_cs,current_rs)\n",
    "    return current_cs,current_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5. Use the K-Means result from Step 4 to generate the DS clusters (i.e.,discard their points and generate statistics).\n",
    "labels = kmeans_2.labels_\n",
    "DS,RS = generate_DS_RS(sample_2,labels,DS,RS)\n",
    "(len(DS),len(RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6. Run K-Means on the points in the RS with a large K to generate CS and RS\n",
    "CS,RS = generate_CS_RS(CS,RS)\n",
    "(len(CS),len(RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ÄúThe intermediate results‚Äù\n",
    "# Round {ùëñ}: the numbers in the order of ‚Äúthe number of the discard points‚Äù,\n",
    "# ‚Äúthe number of the clusters in the compression set‚Äù,‚Äúthe number of the compression points‚Äù,\n",
    "# ‚Äúthe number of the points in the retained set‚Äù.\n",
    "# Round {ùëñ}: ds_len,cs_num,cs_len,rs_num\n",
    "\n",
    "def intermidiate_res(current_ds,current_cs,current_rs):\n",
    "    ds_members = [info[0] for info in current_ds.values()]\n",
    "    ds_len = sum([len(member) for member in ds_members])\n",
    "    \n",
    "    cs_members = [info[0] for info in current_cs.values()]\n",
    "    cs_num = len(current_cs)\n",
    "    cs_len = sum([len(member) for member in cs_members])\n",
    "\n",
    "    rs_num = len(current_rs)\n",
    "    return (ds_len,cs_num,cs_len,rs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = intermidiate_res(DS,CS,RS)\n",
    "final_res = []\n",
    "final_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ma_dist(cluster_centroid,cluster_statistics):\n",
    "    cluster_n = cluster_statistics[1]\n",
    "    cluster_sum = np.array(cluster_statistics[2])\n",
    "    cluster_sumsq = np.array(cluster_statistics[3])\n",
    "    centroid = cluster_sum/cluster_n\n",
    "    sigma = np.sqrt((cluster_sumsq/cluster_n)-centroid**2)\n",
    "    dist = np.sqrt((((cluster_centroid-centroid)/sigma)**2).sum())\n",
    "    return dist\n",
    "\n",
    "def merge_cs(current_cs):\n",
    "    cs = copy.deepcopy(current_cs)\n",
    "    merged = set()\n",
    "    for index_1 in cs:\n",
    "        info_1 = cs[index_1]\n",
    "        length_1 = info_1[1]\n",
    "        sum_1 = info_1[2]\n",
    "        sumsq_1 = info_1[3]\n",
    "        centroid_1 = sum_1/length_1\n",
    "        dimension_1 = len(centroid_1)\n",
    "        dist_min = 100000\n",
    "        neareast_cluster = None\n",
    "        # go through cs and find neareast cluster\n",
    "        for index_2 in cs:\n",
    "            if index_2 != index_1 and index_2 not in merged:\n",
    "                dist_btw_cluster = cal_ma_dist(centroid_1,cs[index_2])\n",
    "                if dist_btw_cluster <= dist_min:\n",
    "                    dist_min = dist_btw_cluster\n",
    "                    neareast_cluster = index_2\n",
    "        # merge and update\n",
    "        if dist_min <= 2*math.sqrt(dimension_1):\n",
    "            cluster_info = current_cs[neareast_cluster]\n",
    "            member_updated = cluster_info[0]+info_1[0]\n",
    "            n_updated = cluster_info[1]+length_1\n",
    "            sum_updated = cluster_info[2]+sum_1\n",
    "            sumsq_updated = cluster_info[3]+sumsq_1\n",
    "            current_cs[neareast_cluster] = [member_updated,n_updated,sum_updated,sumsq_updated]\n",
    "            merged.add(index_1)\n",
    "    # delete merged clusters from cs\n",
    "    for cluster in merged:\n",
    "        cs.pop(cluster)\n",
    "    return cs\n",
    "\n",
    "def assignToSets(sample,current_ds,current_cs,current_rs):\n",
    "    for point in sample:\n",
    "        features = np.array(sample[point])\n",
    "        dimension = len(features)\n",
    "        dist_min = 1000000\n",
    "        cluster_assign_to_ds = None\n",
    "\n",
    "        for cluster in current_ds:\n",
    "            dist = cal_ma_dist(features,current_ds[cluster])\n",
    "            if dist <= dist_min:\n",
    "                dist_min = dist\n",
    "                cluster_assign_to_ds = cluster\n",
    "        \n",
    "        # Step 8. compare using the Mahalanobis Distance and assign to the nearest DS clusters if the distance is<sprt(2ùëë)\n",
    "        if dist_min <= 2*math.sqrt(dimension):\n",
    "            cluster_info_ds = current_ds[cluster_assign_to_ds]\n",
    "            member_updated = cluster_info_ds[0]+[point]\n",
    "            n_updated = cluster_info_ds[1]+1\n",
    "            sum_updated = cluster_info_ds[2]+features\n",
    "            sumsq_updated = cluster_info_ds[3]+(features**2)\n",
    "            current_ds[cluster_assign_to_ds] = [member_updated,n_updated,sum_updated,sumsq_updated]\n",
    "        else: # Step 9. For that are not assigned to DS, ssign them to the nearest CS clusters if the distance is<sprt(2ùëë)\n",
    "            dist_min_cs = 1000000\n",
    "            cluster_assign_to_cs = None\n",
    "\n",
    "            for cluster in current_cs:\n",
    "                dist = cal_ma_dist(features,current_cs[cluster])\n",
    "                if dist<=dist_min_cs:\n",
    "                    dist_min_cs = dist\n",
    "                    cluster_assign_to_cs = cluster\n",
    "            if dist_min_cs <= 2*math.sqrt(dimension):\n",
    "                cluster_info_cs = current_cs[cluster_assign_to_cs]\n",
    "                member_updated = cluster_info_cs[0]+[point]\n",
    "                n_updated = cluster_info_cs[1]+1\n",
    "                sum_updated = cluster_info_cs[2]+features\n",
    "                sumsq_updated = cluster_info_cs[3]+(features**2)\n",
    "                current_cs[cluster_assign_to_cs] = [member_updated,n_updated,sum_updated,sumsq_updated]\n",
    "            else:#  Step 10. For not assigned to a DS cluster or a CS cluster,assign them to RS.\n",
    "                current_rs[point] = sample[point]\n",
    "    # Step 11. Run K-Means on the RS with a large K to generate CS and RS \n",
    "    current_cs,current_rs = generate_CS_RS(current_cs,current_rs)\n",
    "    # Step12.Merge CS clusters that have a Mahalanobis Distance<sprt(2ùëë)\n",
    "    current_cs = merge_cs(current_cs)\n",
    "    return current_ds,current_cs,current_rs\n",
    "\n",
    "def final_merge(current_ds,current_cs):\n",
    "    for cs in current_cs:\n",
    "        cluster_info = current_cs[cs]\n",
    "        cluster_n = cluster_info[1]\n",
    "        cluster_sum = cluster_info[2]\n",
    "        cluster_sumsq = cluster_info[3]\n",
    "        centroid = cluster_sum/cluster_n\n",
    "        dimension = len(centroid)\n",
    "        dist_min = 100000\n",
    "        neareast_cluster = None\n",
    "        # go through ds and find neareast cluster\n",
    "        for ds in current_ds:\n",
    "            dist_btw_cluster = cal_ma_dist(centroid,current_ds[ds])\n",
    "            if dist_btw_cluster<=dist_min:\n",
    "                dist_min = dist_btw_cluster\n",
    "                neareast_cluster = ds\n",
    "        # merge and update\n",
    "        if dist_min <= 2*math.sqrt(dimension):\n",
    "            cluster_info = current_ds[neareast_cluster]\n",
    "            member_updated = cluster_info[0]+cluster_info[0]\n",
    "            n_updated = cluster_info[1]+cluster_n\n",
    "            sum_updated = cluster_info[2]+cluster_sum\n",
    "            sumsq_updated = cluster_info[3]+cluster_sumsq\n",
    "            current_ds[neareast_cluster] = [member_updated,n_updated,sum_updated,sumsq_updated]\n",
    "            current_cs.pop(cs)\n",
    "    return current_ds,current_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round finished\n",
      "(184108, 0, 0, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round finished\n",
      "(230135, 0, 0, 87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round finished\n",
      "(276165, 0, 0, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round finished\n",
      "(322199, 0, 0, 113)\n"
     ]
    }
   ],
   "source": [
    "hash_parts = [0,1,2,3,4]\n",
    "hash_parts.pop(random_num)\n",
    "cnt = 0\n",
    "test_ds = copy.deepcopy(DS)\n",
    "test_cs = copy.deepcopy(CS)\n",
    "test_rs = copy.deepcopy(RS)\n",
    "# Step 7. Load another 20% of the data randomly.\n",
    "for num in hash_parts:\n",
    "    random_sample = data.filter(lambda x: x[1]==num).map(lambda x: (x[0],x[2])).collectAsMap()\n",
    "    # step8-12: For the new points, compare and assign to DS,CS,RS\n",
    "    # DS,CS,RS = assignToSets(random_sample,DS,CS,RS)\n",
    "    test_ds,test_cs,test_rs = assignToSets(random_sample,test_ds,test_cs,test_rs)\n",
    "    cnt += 1\n",
    "    # in last round, merge all cs and all rs into neareast cluster\n",
    "    # if cnt == 4:\n",
    "    #     DS,CS = final_merge(DS,CS)\n",
    "    # res = intermidiate_res(DS,CS,RS)\n",
    "    if cnt == 4:\n",
    "        test_ds,test_cs = final_merge(test_ds,test_cs)\n",
    "    res = intermidiate_res(test_ds,test_cs,test_rs)\n",
    "    final_res.append(res)\n",
    "    print(\"round finished\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996494080270049\n"
     ]
    }
   ],
   "source": [
    "# check Percentage of discard points after last round: >98.5%\n",
    "# print(final_res)\n",
    "print(final_res[-1][0]/len(data.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clustering results\n",
    "# data points index and their clustering results after the BFR algorithm\n",
    "# clustering results should be in [0,the number of clusters),cluster of outliers should be represented as -1.\n",
    "final_res_2 = []\n",
    "for cluster in DS:\n",
    "    for member in DS[cluster][0]:\n",
    "        final_res_2.append((member,cluster))\n",
    "\n",
    "if len(CS) != 0:\n",
    "    for cluster in CS:\n",
    "        for member in CS[cluster][0]:\n",
    "            final_res_2.append((member,-1))\n",
    "\n",
    "if len(RS) != 0:\n",
    "    for member in RS:\n",
    "        final_res_2.append((member,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by data index\n",
    "final_res_2 = sorted(final_res_2,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of your clustering results to the ground truth: 98.0%\n",
    "# from sklearn.metrics import normalized_mutual_info_score\n",
    "# ground_truth = np.loadtxt(input_path, delimiter=\",\")\n",
    "# normalized_mutual_info_score(ground_truth[:,1], np.array(final_res_2)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 600 second\n",
    "# e_time = time.time()\n",
    "# duration = e_time-s_time\n",
    "# print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to txt,format:\n",
    "with open(output_path,\"w\") as f:\n",
    "    f.write(\"The intermediate results:\\n\")\n",
    "    for i in range(len(final_res)):\n",
    "        round = final_res[i]\n",
    "        output = \"Round{\"+str(i+1)+\"}: \"+str(round[0])+\",\"+str(round[1])+\",\"+str(round[2])+\",\"+str(round[3])+\"\\n\"\n",
    "        # print(output)\n",
    "        f.write(output)\n",
    "    # f.write(\"\\n\")\n",
    "    f.write(\"\\nThe clustering results:\\n\")\n",
    "    for i in range(len(final_res_2)):\n",
    "        output = str(final_res_2[i][0])+\",\"+str(final_res_2[i][1])+\"\\n\"\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "# /opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit \n",
    "# --executor-memory 4G --driver-memory 4G \n",
    "# task.py 50 \"../resource/asnlib/publicdata/hw6_clustering.txt\" \"./task.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
