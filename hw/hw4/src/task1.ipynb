{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import GraphFrame\n",
    "import time\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community detection based on GraphFrames\n",
    "# task1.py <filter_threshold> <input_file_path> <output_file_path>\n",
    "# threshold = int(sys.argv[1])\n",
    "# input_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "threshold = 2\n",
    "input_path = \"../data/input/ub_sample_data.csv\"\n",
    "output_path = \"../data/output/task1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/04/01 16:08:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task1\").getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read data and exclude the first line of name\n",
    "# data = sc.textFile(input_path)\n",
    "# head = data.first()\n",
    "# data = data.filter(lambda x: x!=head)\n",
    "data = sc.textFile(input_path).filter(lambda x: x!=\"user_id,business_id\") \n",
    "uid_bids = data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: set(x))\n",
    "# filter first time and construct {uid:[bid,bid,...]}\n",
    "uid_bids_dict = uid_bids.filter(lambda x: len(x[1])>=threshold).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct nodes and edges\n",
    "# node-->user, edge-->exist if two nodes\" common bids >= threshold\n",
    "nodes = set()\n",
    "edges = set()\n",
    "\n",
    "# get user pairs\n",
    "uids = list(uid_bids_dict.keys())\n",
    "user_pairs = combinations(uids,2)\n",
    "# then caculate len(common_bids) and select those cnt>=threshold\n",
    "valid_pairs = []\n",
    "for i in user_pairs:\n",
    "    len_common_bids = len(uid_bids_dict[i[0]].intersection(uid_bids_dict[i[1]]))\n",
    "    if len_common_bids>=threshold:\n",
    "        valid_pairs.append((sorted(i),len_common_bids))\n",
    "\n",
    "users = set()\n",
    "for pair in valid_pairs:\n",
    "    users.add(tuple(pair[0])[0])\n",
    "    users.add(tuple(pair[0])[1])\n",
    "nodes = [(user,) for user in sorted(users)]\n",
    "\n",
    "edges = []\n",
    "for i in valid_pairs:\n",
    "    pair = tuple(i[0])\n",
    "    edges.append(pair)\n",
    "    edges.append(tuple(reversed(pair)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform nodes and edges to dataframe\n",
    "sqlContext = SQLContext(sc)\n",
    "df_nodes = sqlContext.createDataFrame(nodes,[\"id\"])\n",
    "df_edges = sqlContext.createDataFrame(edges,[\"src\", \"dst\"])\n",
    "\n",
    "# consrtuct graph\n",
    "graph = GraphFrame(df_nodes,df_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.labelPropagation(maxIter=5)\n",
    "res = graph.rdd.map(lambda x: (x[1],[x[0]])).reduceByKey(lambda x,y: x+y).map(lambda x: sorted(x[1]))\n",
    "# sort by size, then first uid lexicographical\n",
    "res = res.sortBy(lambda x: (len(x), x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 12.613194942474365\n"
     ]
    }
   ],
   "source": [
    "# less than 100 second\n",
    "# e_time = time.time()\n",
    "# duration = e_time-s_time\n",
    "# print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to txt, format: uid1, uid2, uid3,...\n",
    "with open(output_path,\"w\") as f:\n",
    "    for i in res:\n",
    "        output = \"\"\n",
    "        for node in i:\n",
    "            output = output+\"'\"+str(node)+\", \"\n",
    "        f.write(output[:-2]+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "\n",
    "# /opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit \n",
    "# --packages graphframes:graphframes:0.8.2-spark3.1-s_2.12 --executor-memory 4G --driver-memory 4G \n",
    "# task1.py 2 \"../resource/asnlib/publicdata/ub_sample_data.csv\" \"./task1.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
