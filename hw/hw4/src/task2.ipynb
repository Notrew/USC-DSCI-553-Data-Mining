{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community detection based on GraphFrames\n",
    "# task2.py <filter_threshold> <input_file_path> <output_file_path_1> <output_file_path_2>\n",
    "# threshold = int(sys.argv[1])\n",
    "# input_path = sys.argv[2]\n",
    "# output_path_1 = sys.argv[3]\n",
    "# output_path_2 = sys.argv[4]\n",
    "threshold = 5\n",
    "input_path = \"../data/input/ub_sample_data.csv\"\n",
    "output_path_1 = \"../data/output/betweenness.txt\"\n",
    "output_path_2 = \"../data/output/community.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/04/06 23:05:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task1\").getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read data and exclude the first line of name\n",
    "# data = sc.textFile(input_path)\n",
    "# head = data.first()\n",
    "# data = data.filter(lambda x: x!=head)\n",
    "data = sc.textFile(input_path).filter(lambda x: x!=\"user_id,business_id\") \n",
    "uid_bids = data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: set(x))\n",
    "# filter first time and construct {uid:[bid,bid,...]}\n",
    "uid_bids_dict = uid_bids.filter(lambda x: len(x[1])>=threshold).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct nodes and edges\n",
    "# node-->user, edge-->exist if two nodes\" common bids >= threshold\n",
    "\n",
    "# get user pairs\n",
    "uids = list(uid_bids_dict.keys())\n",
    "user_pairs = combinations(uids,2)\n",
    "# then caculate len(common_bids) and select those cnt>=threshold\n",
    "valid_pairs_len = []\n",
    "for i in user_pairs:\n",
    "    len_common_bids = len(uid_bids_dict[i[0]].intersection(uid_bids_dict[i[1]]))\n",
    "    if len_common_bids>=threshold:\n",
    "        valid_pairs_len.append((sorted(i),len_common_bids))\n",
    "\n",
    "nodes = set()\n",
    "for pair in valid_pairs_len:\n",
    "    nodes.add(tuple(pair[0])[0])\n",
    "    nodes.add(tuple(pair[0])[1])\n",
    "\n",
    "connections = defaultdict(set)\n",
    "for pairs_len in valid_pairs_len:\n",
    "    pairs = pairs_len[0]\n",
    "    connections[pairs[0]].add(pairs[1])\n",
    "    connections[pairs[1]].add(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girvan-Newman Alg\n",
    "# visit each node X once (BFS)\n",
    "# compute the # of the shortest paths from X to each of the other nodes\n",
    "# repeat:\n",
    "    # calculate betweenness of edges, and remove high betweennedd edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(root,connections):\n",
    "    parents_lst = defaultdict(list)\n",
    "    depth = defaultdict(int)\n",
    "    num_shortest_path = defaultdict(int)\n",
    "    queue = []\n",
    "    bfs_queue = []\n",
    "\n",
    "    # set default value\n",
    "    parents_lst[root] = None\n",
    "    depth[root] = 0\n",
    "    num_shortest_path[root] = 1\n",
    "    bfs_queue.append(root)\n",
    "\n",
    "    # prepare children of root\n",
    "    for adjacent in connections[root]:\n",
    "        parents_lst[adjacent] = [root]\n",
    "        depth[adjacent] = 1\n",
    "        num_shortest_path[adjacent] = 1\n",
    "        bfs_queue.append(adjacent)\n",
    "        queue.append(adjacent)\n",
    "\n",
    "    while queue:\n",
    "        cur_node = queue.pop(0)\n",
    "        # go through neighbours\n",
    "        for adjacent in connections[cur_node]:\n",
    "            # if it didn\"t appear before, set it as cur_node\"s child\n",
    "            if adjacent not in bfs_queue:\n",
    "                parents_lst[adjacent] = [cur_node]\n",
    "                depth[adjacent] = depth[cur_node]+1\n",
    "                bfs_queue.append(adjacent)\n",
    "                queue.append(adjacent)\n",
    "            # it appeared before\n",
    "            else:\n",
    "                if depth[adjacent]==depth[cur_node]+1:\n",
    "                    parents_lst[adjacent].append(cur_node)\n",
    "        num_shortest_path[cur_node] = sum(num_shortest_path[parent] for parent in parents_lst[cur_node])\n",
    "    bfs_queue.reverse()\n",
    "    return bfs_queue,parents_lst,num_shortest_path\n",
    "\n",
    "def cal_credit(bfs_queue_rever,parents_lst,num_shortest_path):\n",
    "    # set default credit\n",
    "    basic_credit = {}\n",
    "    # credit of root = 0\n",
    "    basic_credit[bfs_queue_rever[-1]] = 0\n",
    "    # else = 1 at beginning\n",
    "    for node in bfs_queue_rever[:-1]:\n",
    "        basic_credit[node] = 1\n",
    "\n",
    "    credit_dict = {}\n",
    "    # form bottom to\n",
    "    for child in bfs_queue_rever[:-1]:\n",
    "        for parent in parents_lst[child]:\n",
    "            weight = num_shortest_path[parent]/num_shortest_path[child]\n",
    "            credit = basic_credit[child]*weight\n",
    "            basic_credit[parent] += credit\n",
    "            credit_dict[tuple(sorted((child,parent)))] = credit\n",
    "\n",
    "    return [(pair,credit) for pair,credit in credit_dict.items()]\n",
    "\n",
    "def GN_Alg(root,connections):\n",
    "    bfs_res = bfs(root,connections)\n",
    "    credit_res = cal_credit(bfs_res[0],bfs_res[1],bfs_res[2])\n",
    "    return credit_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate betweenness\n",
    "betweenness = sc.parallelize(nodes).map(lambda x: GN_Alg(x,connections)).flatMap(lambda x: x).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0],x[1]/2))\n",
    "betweenness = betweenness.map(lambda x: (x[0],round(x[1],5))).collect()\n",
    "betweenness = sorted(betweenness,key=lambda x: (-x[1],x[0][0],x[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to txt, format: (uid1, uid2), betweenness\n",
    "with open(output_path_1,\"w\") as f:\n",
    "    for i in betweenness:\n",
    "        output = \"(\"\"+i[0][0]+\"\", \"\"+i[0][1]+\"\"),\"+str(i[1])+\"\\n\"\n",
    "        f.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect communities\n",
    "def min_cut(pair,connections):\n",
    "    p1 = pair[0]\n",
    "    p2 = pair[1]\n",
    "    # update the connections\n",
    "    # print(p1,p2)\n",
    "    connections[p1].remove(p2)\n",
    "    connections[p2].remove(p1)\n",
    "\n",
    "def get_community(all_nodes,connections):\n",
    "    communities = []\n",
    "    queue = []\n",
    "    nodes_visited = []\n",
    "    for node in all_nodes:\n",
    "        if node not in nodes_visited:\n",
    "            tmp = [node]\n",
    "            queue.append(node)\n",
    "            while queue:\n",
    "                cur_node = queue.pop(0)\n",
    "                for adjacent in connections[cur_node]:\n",
    "                    if adjacent not in tmp:\n",
    "                        tmp.append(adjacent)\n",
    "                        queue.append(adjacent)\n",
    "            tmp.sort()\n",
    "            nodes_visited += tmp\n",
    "            communities.append(tmp)\n",
    "    return communities\n",
    "\n",
    "def cal_modularity(communities):\n",
    "    org_modularity = 0\n",
    "    for community in communities:\n",
    "        for i in community:\n",
    "            for j in community:\n",
    "                actual = 1 if j in connections[i] else 0\n",
    "                expected = (degree_info[i]*degree_info[j])/(2*m)\n",
    "                org_modularity += (actual - expected)\n",
    "    return org_modularity/(2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# num of edges\n",
    "m = len(valid_pairs_len)\n",
    "# degree\n",
    "degree_info = {}\n",
    "for node in connections:\n",
    "    degree_info[node] = len(connections[node])\n",
    "\n",
    "# if still has edges can cut\n",
    "connections_info = copy.deepcopy(connections)\n",
    "total_modularity = -1\n",
    "remaining_edges = m\n",
    "while remaining_edges>0:\n",
    "    highest_betweenness = betweenness[0][1]\n",
    "    # cut\n",
    "    for pair_betweenness in betweenness:\n",
    "        if pair_betweenness[1] == highest_betweenness:\n",
    "            # print(pair_betweenness[0])\n",
    "            min_cut(pair_betweenness[0],connections_info)\n",
    "            # update remaining_edges\n",
    "            remaining_edges -= 1\n",
    "    # update\n",
    "    community_after_cut = get_community(nodes,connections_info)\n",
    "    modularity = cal_modularity(community_after_cut)\n",
    "    if modularity>total_modularity:\n",
    "        # update modularity\n",
    "        total_modularity = modularity\n",
    "        communities_res = copy.deepcopy(community_after_cut)\n",
    "    # recalculate betweenness\n",
    "    betweenness = sc.parallelize(nodes).map(lambda x: GN_Alg(x,connections_info)).flatMap(lambda x: x).reduceByKey(lambda x,y: x+y)\\\n",
    "        .map(lambda x: (sorted(x[0]),x[1]/2)).sortBy(lambda x: (-x[1],x[0][0],x[0][1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sc.parallelize(communities_res).map(lambda x: sorted(x)).sortBy(lambda x: (len(x),x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path_2, \"w\") as f:\n",
    "    for i in res:\n",
    "        f.write(str(i).strip(\"[\").strip(\"]\")+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 294.4706311225891\n"
     ]
    }
   ],
   "source": [
    "# less than 400 second\n",
    "e_time = time.time()\n",
    "duration = e_time-s_time\n",
    "print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "\n",
    "# /opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit \n",
    "# --executor-memory 4G --driver-memory 4G \n",
    "# task2.py 5 \"../resource/asnlib/publicdata/ub_sample_data.csv\" \"./task2_1.txt\" \"./task2_2.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
