{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb176c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95891c3a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/03 17:59:05 WARN Utils: Your hostname, NotredeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.26.112.211 instead (on interface en0)\n",
      "23/02/03 17:59:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/02/03 17:59:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# task2, input format: task2.py <review_filepath> <output_filepath> <n_partition>\n",
    "# review_path = sys.argv[1]\n",
    "# output_path = sys.argv[2]\n",
    "# n_partition = int(sys.argv[3])\n",
    "review_path = \"./data/test_review.json\"\n",
    "output_path = \"./data/task2.json\"\n",
    "n_partition = 4\n",
    "sc = SparkContext(\"local[*]\",appName=\"task2\").getOrCreate()\n",
    "review = sc.textFile(review_path).map(lambda x: json.loads(x))\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1725e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default\n",
    "default = {}\n",
    "s_time = time.time()\n",
    "review.map(lambda x: [x[\"business_id\"],1]).reduceByKey(lambda x,y: x+y).sortBy(lambda x:[-x[1],x[0]]).take(10)\n",
    "e_time = time.time()\n",
    "\n",
    "default[\"n_partition\"] = review.getNumPartitions()\n",
    "default[\"n_items\"] = review.glom().map(lambda x: len(x)).collect()\n",
    "# default[\"n_items\"] = review.mapPartitions(lambda x: [len(list(x))]).collect()\n",
    "default[\"exe_time\"] = e_time-s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12c1cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized\n",
    "customized = {}\n",
    "s_time = time.time()\n",
    "review_new = review.map(lambda x: (x['business_id'], 1)).partitionBy(n_partition, lambda x: ord(x[0]))\n",
    "review_new.reduceByKey(lambda x,y: x+y).sortBy(lambda x:[-x[1],x[0]]).take(10)\n",
    "e_time = time.time()\n",
    "\n",
    "customized[\"n_partition\"] = review_new.getNumPartitions()\n",
    "customized[\"n_items\"] = review_new.glom().map(lambda x: len(x)).collect()\n",
    "customized[\"exe_time\"] = e_time-s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f6f7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': {'n_partition': 2, 'n_items': [13, 17], 'exe_time': 0.12612104415893555}, 'customized': {'n_partition': 3, 'n_items': [10, 7, 13], 'exe_time': 0.1925191879272461}}\n"
     ]
    }
   ],
   "source": [
    "output[\"default\"] = default\n",
    "output[\"customized\"] = customized\n",
    "print(output)\n",
    "with open(output_path,\"w\") as f:\n",
    "    json.dump(output,f) \n",
    "\n",
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "#/opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --executor-memory 4G --driver-memory 4G task2.py\n",
    "# \"../resource/asnlib/publicdata/test_review.json\" \"./task2.json\" 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
