{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from itertools import combinations\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LSH with Jaccard similarity on yelp_train.csv\n",
    "# task1.py <input_file_path> <output_file_path>\n",
    "# input_path = sys.argv[1]\n",
    "# output_path = sys.argv[2]\n",
    "input_path = \"../data/input/yelp_train.csv\"\n",
    "output_path = \"../data/output/task1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design hash function to shuffle uids\n",
    "def shuffleIndex(shuffle_times,indexToShuffle):\n",
    "    index_shuffled_n_times = []\n",
    "    # f(x) = ((ax + b) % p) % m\n",
    "    a = random.sample(range(1,round(time.time()/10000)),shuffle_times)\n",
    "    b = random.sample(range(1,round(time.time()/10000)),shuffle_times)\n",
    "    p = 9965\n",
    "    m = len(indexToShuffle)+1 #number of bins\n",
    "    for i_th_shuffle in range(shuffle_times):\n",
    "        new_index_l = []\n",
    "        for index in range(len(indexToShuffle)):\n",
    "            new_inex  = ((a[i_th_shuffle]*index + b[i_th_shuffle])%p)%m\n",
    "            new_index_l.append(new_inex)\n",
    "        index_shuffled_n_times.append(new_index_l)\n",
    "    return index_shuffled_n_times\n",
    "\n",
    "# select the minimum uid index of one hashed results for one bid\n",
    "# and then to build signature matrix\n",
    "def minHash(valid_item_l,org_index_l,index_shuffled_n_times):\n",
    "    # get original index of uid\n",
    "    org_index = [org_index_l[i] for i in valid_item_l]\n",
    "    sig = []\n",
    "    for i_th_shuffle in range(len(index_shuffled_n_times)):\n",
    "        new_index = []\n",
    "        for index in org_index:\n",
    "            new_index.append(index_shuffled_n_times[i_th_shuffle][index])\n",
    "        sig.append(min(new_index))\n",
    "    return sig\n",
    "\n",
    "def genCandPairs(b_num,r_num,list_of_uid_list):\n",
    "    res = set()\n",
    "    # divide sig_matrix into b bands\n",
    "    for band_i in range(b_num):\n",
    "        start_row = int(band_i*2)\n",
    "        in_one_band = {}  # {[uid,uid,...]:[bid,bid,...]}\n",
    "        for i_th_bid_index in range(len(list_of_uid_list)):\n",
    "            its_uid_indexes = list_of_uid_list[i_th_bid_index]\n",
    "            portion_uid_indexes = tuple(its_uid_indexes[start_row:start_row+r_num]) #list is unhashable\n",
    "            # for each band, hash bids with same portion of uids in to one bucket\n",
    "            if portion_uid_indexes in in_one_band:\n",
    "                in_one_band[portion_uid_indexes].append(i_th_bid_index)\n",
    "            else:\n",
    "                in_one_band[portion_uid_indexes] = [i_th_bid_index]\n",
    "        # candidate paris are those that hash to same buckets more than one bands\n",
    "        # in in_one_band dict, bids in one list are possible sigletons of candidate pairs\n",
    "        for bid_l in list(in_one_band.values()):\n",
    "            if len(bid_l) >= 2:\n",
    "                pairs = combinations(bid_l,2)\n",
    "                # sort and save in res set\n",
    "                for pair in pairs:\n",
    "                    res.add(tuple(sorted(pair)))\n",
    "        # do combinations after going through all bands and removing duplicates\n",
    "        # try it latter\n",
    "    return res\n",
    "\n",
    "def cal_sim_and_filter(candidate_pairs,threshold):\n",
    "    res = []\n",
    "    for pair in candidate_pairs:\n",
    "        # # get corresponding uid list according to bid\n",
    "        # uids_of_bid_1 = bid_uids_info_dict[bid_1]\n",
    "        # uids_of_bid_2 = bid_uids_info_dict[bid_2]\n",
    "        # get corresponding uid list according to bid index\n",
    "        uids_of_bid_1 = set(bid_index_uids_info_dict[pair[0]])\n",
    "        uids_of_bid_2 = set(bid_index_uids_info_dict[pair[1]])\n",
    "        # calculate jaccard similarity\n",
    "        j_sim = len(uids_of_bid_1.intersection(uids_of_bid_2))/len(uids_of_bid_1.union(uids_of_bid_2))\n",
    "        # filter those j_sim >= threshold\n",
    "        if j_sim >= threshold:\n",
    "            # get corresponding bid\n",
    "            bid_1 = bid_index[pair[0]]\n",
    "            bid_2 = bid_index[pair[1]]\n",
    "            res.append(sorted([bid_1,bid_2])+[j_sim])\n",
    "    return sorted(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 16:31:50 WARN Utils: Your hostname, NotredeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.26.23.135 instead (on interface en0)\n",
      "23/03/09 16:31:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/09 16:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(input_path).filter(lambda x: x!=\"user_id,business_id,stars\") #exclude the first line of name\n",
    "# split one row into (uid,bid)\n",
    "uid_bid = data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1]))\n",
    "# combine uid of the same bid into a list==>(bid,[uid,uid,...])\n",
    "bid_uids = uid_bid.map(lambda x: (x[1],[x[0]])).reduceByKey(lambda x,y: x+y)\n",
    "# remove the dupplicates\n",
    "bid_uids = bid_uids.mapValues(lambda x: [*set(x)])\n",
    "\n",
    "# # get bid list and bid index, used to get bid from index\n",
    "bids = uid_bid.map(lambda x: x[1]).distinct().collect()\n",
    "bid_index = {}\n",
    "for i in range(len(bids)):\n",
    "    bid_index[i] = bids[i]\n",
    "\n",
    "# get uid list and uid index, shuffle latter\n",
    "uids = uid_bid.map(lambda x: x[0]).distinct().collect()\n",
    "uid_index = {}\n",
    "for i in range(len(uids)):\n",
    "    uid_index[uids[i]] = i\n",
    "\n",
    "# # get bid and corresponding uid list into dict, used to calculate simmilarity according to bid\n",
    "# bid_uids_info_list = bid_uids.collect()\n",
    "# bid_uids_info_dict = {}\n",
    "# for i in range(len(bid_uids_info_list)):\n",
    "#     bid_uids_info_dict[bid_uids_info_list[i][0]] = bid_uids_info_list[i][1]\n",
    "\n",
    "# get bid_index and corresponding uid list into dict, used to calculate simmilarity according to bid_index\n",
    "bid_uids_info_list = bid_uids.collect()\n",
    "bid_index_uids_info_dict = {}\n",
    "for i in range(len(bid_uids_info_list)):\n",
    "    bid_index_uids_info_dict[i] = bid_uids_info_list[i][1]\n",
    "\n",
    "# hash index\n",
    "shuffle_times = 50\n",
    "index_shuffled_50_times = shuffleIndex(shuffle_times,uids)\n",
    "\n",
    "# minhash, generage signature matrix (50 x len_of_bid)\n",
    "sig_matrix = bid_uids.map(lambda x: (x[0],minHash(x[1],uid_index,index_shuffled_50_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH\n",
    "# combine uid lists into one list\n",
    "sig_matrix_comb_uid = sig_matrix.map(lambda x: (0,x[1])).groupByKey().map(lambda x: list(x[1]))\n",
    "# generage candidate pairs, (bid_index,bid_index)\n",
    "# b bands and r rows, b*r=n(number of hash functions)\n",
    "b_num = 25\n",
    "r_num = 2\n",
    "# genCandPairs(b_num,r_num,sig_matrix_comb_uid.collect()[0])\n",
    "candidate_pairs = sig_matrix_comb_uid.map(lambda x: genCandPairs(b_num,r_num,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(bid_index)):\n",
    "#     while not bid_index_uids_info_dict[i]==bid_uids_info_dict[bid_index[i]]:\n",
    "#         print(\"ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filter candidate pairs whose Jaccard similarity is >= 0.5\n",
    "threshold = 0.5\n",
    "pair_sim = candidate_pairs.map(lambda x: cal_sim_and_filter(x,threshold)).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 12.613194942474365\n"
     ]
    }
   ],
   "source": [
    "# less than 100 second\n",
    "e_time = time.time()\n",
    "duration = e_time-s_time\n",
    "print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv, header: business_id_1, business_id_2, similarity\n",
    "with open(output_path,\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"business_id_1\",\"business_id_2\",\"similarity\"])\n",
    "    for i in pair_sim:\n",
    "        # print(i)\n",
    "        writer.writerow(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision and recall score based on ground truth file “pure_jaccard_similarity.csv”\n",
    "# precision >= 0.99 and recall >= 0.9\n",
    "# Precision = true positives / (true positives + false positives) \n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# task1 = pd.read_csv(output_path)\n",
    "# truth = pd.read_csv(\"../data/input/pure_jaccard_similarity.csv\")\n",
    "# task1[\"comb\"] = task1.apply(lambda row: row[\"business_id_1\"]+row[\"business_id_2\"]+str(row[\"similarity\"]),axis = 1)\n",
    "# truth[\"comb\"] = truth.apply(lambda row: row[\"business_id_1\"]+row[\" business_id_2\"]+str(row[\" similarity\"]),axis = 1)\n",
    "# print(precision_score(truth[\"comb\"],task1[\"comb\"],average='macro'))\n",
    "# print(precision_score(truth[\"comb\"],task1[\"comb\"],average='micro'))\n",
    "# print(precision_score(truth[\"comb\"],task1[\"comb\"],average=\"weighted\"))\n",
    "# # Recall = true positives / (true positives + false negatives)\n",
    "# print(recall_score(truth[\"comb\"],task1[\"comb\"],average='macro'))\n",
    "# print(recall_score(truth[\"comb\"],task1[\"comb\"],average='micro'))\n",
    "# print(recall_score(truth[\"comb\"],task1[\"comb\"],average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "#/opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --executor-memory 4G --driver-memory 4G task1.py\n",
    "# \"../resource/asnlib/publicdata/yelp_train.csv\"\n",
    "# \"./task1.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
