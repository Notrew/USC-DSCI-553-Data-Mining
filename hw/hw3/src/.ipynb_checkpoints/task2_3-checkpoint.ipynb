{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2_3.py <folder_path> <test_file_path> <output_file_path>\n",
    "# folder_path = sys.argv[1]\n",
    "# test_path = sys.argv[2]\n",
    "# output_path = sys.argv[3]\n",
    "folder_path = \"../data/input/\"\n",
    "test_path = \"../data/input/yelp_val.csv\"\n",
    "output_path = \"../data/output/task2_3.csv\"\n",
    "\n",
    "train_path = folder_path+\"yelp_train.csv\"\n",
    "user_path = folder_path+\"user.json\"\n",
    "business_path = folder_path+\"business.json\"\n",
    "review_train_path = folder_path+\"review_train.json\"\n",
    "# checkin_path = folder_path+\"checkin.json\"\n",
    "# tip_path = folder_path+\"tip.json\"\n",
    "photo_path = folder_path+\"photo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/notre/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/20 12:38:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "sc = SparkContext(\"local[*]\",appName=\"task2_3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid = item-based+model-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item-based\n",
    "# Pearson similarity between i and j\n",
    "sim_cache = {} #sim_cache\n",
    "def calSim(bid1,bid2):\n",
    "    # find avg rate for each item--all rated, not co-rated\n",
    "    avg_rate_1 = hist_bid_avg_rate[bid1]\n",
    "    avg_rate_2 = hist_bid_avg_rate[bid2]\n",
    "    user_list_1 = hist_bid_uids_info_dict[bid1]\n",
    "    user_list_2 = hist_bid_uids_info_dict[bid2]\n",
    "    # find users both rated i and j\n",
    "    co_rate_users = set(user_list_1).intersection(set(user_list_2))\n",
    "    # calculate (rate-avg_rate) for each user on i and j\n",
    "    nor_rates = []\n",
    "    for  co_rate_user in co_rate_users:\n",
    "        nor_rate_1 = hist_bid_uid_tuple_rate[tuple([bid1,co_rate_user])]-avg_rate_1\n",
    "        nor_rate_2 = hist_bid_uid_tuple_rate[tuple([bid2,co_rate_user])]-avg_rate_2\n",
    "        nor_rates.append([nor_rate_1,nor_rate_2])\n",
    "    # calculate Pearson similarity\n",
    "    nmr = sum([rate[0]*rate[1] for rate in nor_rates])\n",
    "    dnm = math.sqrt(sum([rate[0]**2 for rate in nor_rates]))*math.sqrt(sum([rate[1]**2 for rate in nor_rates]))\n",
    "    if dnm != 0:\n",
    "        sim = nmr/dnm\n",
    "    else:\n",
    "        sim = 0\n",
    "    pair = tuple(sorted([bid1,bid2]))\n",
    "    if pair not in sim_cache:\n",
    "        sim_cache[pair] = sim\n",
    "    return sim\n",
    "\n",
    "def predict(bid_to_pred,test_bid_uids_info_dict):\n",
    "    # [[uid,bid,pred_rate],...]\n",
    "    res = [] \n",
    "    # new bid, use all users rated this bid and rate=3.0 to build item profile\n",
    "    if bid_to_pred not in hist_bids:\n",
    "        # res = [[uid,bid_to_pred,3.0] for uid in test_bid_uids_info_dict[bid_to_pred]]\n",
    "        # or use avg_rate of this user to fill latter\n",
    "        res = [[uid,bid_to_pred,hist_uid_avg_rate[uid]] for uid in test_bid_uids_info_dict[bid_to_pred]]\n",
    "        return res\n",
    "    users_to_pred = test_bid_uids_info_dict[bid_to_pred]\n",
    "    for user in users_to_pred:\n",
    "        rate_sim = []\n",
    "        # new user, use rate=3.0 to build item profile\n",
    "        if user not in hist_uids:\n",
    "            # res.append([user,bid_to_pred,3.0])\n",
    "            # or use avg_rate of this bid to fill latter\n",
    "            res.append([user,bid_to_pred,hist_bid_avg_rate[bid_to_pred]])\n",
    "            continue\n",
    "        # bid and user both have historical data\n",
    "        # if this user only rated bid_to_pred before, use historical data\n",
    "        if hist_uid_bids_info_dict[user]==[bid_to_pred]:\n",
    "            res.append([user,bid_to_pred,hist_bid_uid_tuple_rate[(user,bid_to_pred)]])\n",
    "            continue\n",
    "        # find possible neighbor/bid\n",
    "        possible_nbors = set(hist_uid_bids_info_dict[user])-set(bid_to_pred)\n",
    "        # find co-rated user of bid_to_pred/i and possible_nbor\n",
    "        for possible_nbor in possible_nbors:\n",
    "            co_rate_users = set(hist_bid_uids_info_dict[bid_to_pred]).intersection(set(hist_bid_uids_info_dict[possible_nbor]))\n",
    "            if not co_rate_users:\n",
    "                continue\n",
    "            else:\n",
    "            # calculate sim \n",
    "                # if alreadey calculated\n",
    "                pair = tuple(sorted([bid_to_pred,possible_nbor]))\n",
    "                if pair in sim_cache:\n",
    "                    sim = sim_cache[pair]\n",
    "                else:\n",
    "                    sim = calSim(bid_to_pred,possible_nbor)\n",
    "                rate_sim.append([hist_bid_uid_tuple_rate[(possible_nbor,user)],sim])    \n",
    "        # select top n neighbors\n",
    "        rate_sim.sort(key=lambda x: x[1],reverse=True)\n",
    "        n = min(20,len(rate_sim))\n",
    "        top_nbor_info = rate_sim[:n]\n",
    "        nmr = sum([info[0]*info[1] for info in top_nbor_info])\n",
    "        dnm = sum([abs(info[1]) for info in top_nbor_info])\n",
    "        # predict\n",
    "        if dnm != 0:\n",
    "            rate_pred = 0.1*nmr/dnm +0.5*hist_bid_avg_rate[bid_to_pred]+0.4*hist_uid_avg_rate[user]\n",
    "            rate_pred = min(5.0,max(0.0,rate_pred))\n",
    "        else:\n",
    "            rate_pred = (hist_bid_avg_rate[bid_to_pred]+hist_uid_avg_rate[user])/2\n",
    "        res.append([user,bid_to_pred,rate_pred])\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read train data and train and get basic info\n",
    "# split one row into (uid,bid,star)\n",
    "train_data = sc.textFile(train_path)\n",
    "head = train_data.first()\n",
    "train_data = train_data.filter(lambda x: x!=head) #exclude the first line of name\n",
    "hist_uid_bid_rate = train_data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1],float(x[2])))\n",
    "# combine bid of the same uid into a list and remove the duplicates\n",
    "# (uid,[bid,bid,...])\n",
    "hist_uid_bids = hist_uid_bid_rate.map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: [*set(x)])\n",
    "# {uid:['bid,bid,...]}, find neighbors\n",
    "hist_uid_bids_info_dict = hist_uid_bids.collectAsMap()\n",
    "hist_uids = list(hist_uid_bids_info_dict.keys())\n",
    "# (bid,[uid,uid,...])\n",
    "hist_bid_uids = hist_uid_bid_rate.map(lambda x: (x[1],[x[0]])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: [*set(x)])\n",
    "# {bid:['uid,uid,...]}, find co-rated users\n",
    "hist_bid_uids_info_dict  = hist_bid_uids.collectAsMap()\n",
    "hist_bids = list(hist_bid_uids_info_dict.keys())\n",
    "# {(bid,uid):score}\n",
    "hist_bid_uid_tuple_rate = hist_uid_bid_rate.map(lambda x: ((x[1],x[0]),x[2])).collectAsMap()\n",
    "# avg rate for each item--all rated, not co-rated\n",
    "# {uid:avg_star}\n",
    "hist_uid_avg_rate = hist_uid_bid_rate.map(lambda x: (x[0],x[2])).groupByKey().map(lambda x: (x[0],sum(list(x[1]))/len(list(x[1])))).collectAsMap()\n",
    "# {bid:avg_star}\n",
    "hist_bid_avg_rate = hist_uid_bid_rate.map(lambda x: (x[1],x[2])).groupByKey().map(lambda x: (x[0],sum(list(x[1]))/len(list(x[1])))).collectAsMap()\n",
    "\n",
    "# read test data and train and get basic info\n",
    "test_data = sc.textFile(test_path)\n",
    "test_head = test_data.first()\n",
    "test_data = test_data.filter(lambda x: x!=test_head) #exclude the first line of name\n",
    "# (bid,[uid,uid,...])\n",
    "bid_uids_to_pred = test_data.map(lambda x: x.split(\",\")).map(lambda x: (x[1],[x[0]])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: [*set(x)])\n",
    "# {bid:['uid,uid,...],...}\n",
    "test_bid_uids_info_dict = bid_uids_to_pred.collectAsMap()\n",
    "\n",
    "after_pred = bid_uids_to_pred.map(lambda x: predict(x[0],test_bid_uids_info_dict)).flatMap(lambda x: x)\n",
    "res = after_pred.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = []\n",
    "business_id = []\n",
    "pred_item = []\n",
    "for i in res:\n",
    "    user_id.append(i[0])\n",
    "    business_id.append(i[1])\n",
    "    pred_item.append(i[2])\n",
    "res_item = pd.DataFrame({\"user_id\":user_id, \"business_id\":business_id, \"prediction\":pred_item})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0490843578388231\n"
     ]
    }
   ],
   "source": [
    "# truth = pd.read_csv(test_path)\n",
    "# merged_item = truth.merge(res_item,on=[\"user_id\", \"business_id\"])\n",
    "# print(mean_squared_error(merged_item[\"stars\"],merged_item[\"prediction\"],squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-based\n",
    "def getPriceRange(attributes,key):\n",
    "    if attributes:\n",
    "        if key in attributes.keys():\n",
    "            return float(attributes.get(key))\n",
    "    return 0\n",
    "\n",
    "# var_rate and photo_cnt may be None\n",
    "def fillInNone(num,default):\n",
    "    if num:\n",
    "        return num\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "# uid_info: {user_id:(review_count,fans,average_stars,friends,social,var_rate)}\n",
    "# bid_info: {business_id:(stars,review_count,price_range,var_rate,phtot_cnt)}\n",
    "# get and join features together, all inputs are dictionaries\n",
    "def mergrFeatures(df_org,uid_info,bid_info):\n",
    "    col_names = [\"user_review_cnt\",\"user_fans\",\"user_avg_rate\",\"user_var_rate\",\n",
    "                 \"user_friends\",\"user_social\",\"user_year\",\"user_elite\",\"user_compliment\",\n",
    "                 \"bsn_avg_rate\",\"bsn_var_rate\",\"bsn_review_cnt\",\"bsn_price_range\",\"bsn_photo_cnt\"]\n",
    "    user_review_cnt = []\n",
    "    user_fans = []\n",
    "    user_avg_rate = []\n",
    "    user_var_rate = []\n",
    "    user_friends = []\n",
    "    user_social = []\n",
    "    user_year = []\n",
    "    user_elite = []\n",
    "    user_compliment = []\n",
    "    bsn_avg_rate = []\n",
    "    bsn_var_rate = []\n",
    "    bsn_review_cnt = []\n",
    "    bsn_price_range = [ ]\n",
    "    bsn_photo_cnt = []\n",
    "    for uid in df_org[\"user_id\"]:\n",
    "        if uid in uid_info.keys():\n",
    "            user_review_cnt.append(uid_info.get(uid)[0])\n",
    "            user_fans.append(uid_info.get(uid)[1])\n",
    "            user_avg_rate.append(uid_info.get(uid)[2])\n",
    "            user_friends.append(uid_info.get(uid)[3])\n",
    "            user_social.append(uid_info.get(uid)[4])\n",
    "            user_year.append(uid_info.get(uid)[5])\n",
    "            user_elite.append(uid_info.get(uid)[6])\n",
    "            user_compliment.append(uid_info.get(uid)[7])\n",
    "            user_var_rate.append(uid_info.get(uid)[8])\n",
    "        else:\n",
    "            user_review_cnt.append(uid_review_cnt_whole)\n",
    "            user_fans.append(uid_fans_whole)\n",
    "            user_avg_rate.append(uid_avg_rate_whole)\n",
    "            user_friends.append(uid_fri_whole)\n",
    "            user_social.append(uid_social_whole)\n",
    "            user_year.append(0)\n",
    "            user_elite.append(0)\n",
    "            user_compliment.append(0)\n",
    "            user_var_rate.append(0)\n",
    "    for bid in df_org[\"business_id\"]:\n",
    "        if bid in bid_info.keys():\n",
    "            bsn_avg_rate.append(bid_info.get(bid)[0])\n",
    "            bsn_var_rate.append(bid_info.get(bid)[3])\n",
    "            bsn_review_cnt.append(bid_info.get(bid)[1])\n",
    "            bsn_price_range.append(bid_info.get(bid)[2])\n",
    "            bsn_photo_cnt.append(bid_info.get(bid)[4])\n",
    "        else:\n",
    "            bsn_avg_rate.append(bid_avg_rate_whole)\n",
    "            bsn_review_cnt.append(bid_review_cnt_whole)\n",
    "            bsn_price_range.append(bid_price_range_whole)\n",
    "            bsn_var_rate.append(0)\n",
    "            bsn_photo_cnt.append(0)\n",
    "            # bsn_avg_rate.append(3)\n",
    "            # bsn_review_cnt.append(0)\n",
    "            # bsn_price_range.append(0)\n",
    "    for i in col_names:\n",
    "        df_org[i] = locals()[i]\n",
    "    return df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre_datasets, select features\n",
    "# train_data\n",
    "train_data = sc.textFile(train_path)\n",
    "head = train_data.first()\n",
    "train_data = train_data.filter(lambda x: x!=head) #exclude the first line of name\n",
    "train_uid_bid_rate = train_data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1],float(x[2])))\n",
    "# hist_uids = hist_uid_bid_rate.map(lambda x: x[0]).distinct()\n",
    "# hist_bids = hist_uid_bid_rate.map(lambda x: x[1]).distinct()\n",
    "\n",
    "# user, select: user_id,(review_count,fans,average_stars,friends,useful+funny+cool,year,elite,compliment))\n",
    "user = sc.textFile(user_path).map(lambda x: json.loads(x))\n",
    "user = user.map(lambda x: (x[\"user_id\"],(x[\"review_count\"],x[\"fans\"],x[\"average_stars\"],len(x[\"friends\"].split(\",\")),\n",
    "                                         x[\"useful\"]+x[\"funny\"]+x[\"cool\"],(2023-int(x[\"yelping_since\"][:4])),len(x[\"elite\"].split(\",\")),\n",
    "                                         x[\"compliment_hot\"]+x[\"compliment_more\"]+x[\"compliment_profile\"]+x[\"compliment_cute\"]+\\\n",
    "                                         x[\"compliment_list\"]+x[\"compliment_note\"]+x[\"compliment_plain\"]+x[\"compliment_cool\"]+\\\n",
    "                                         x[\"compliment_funny\"]+x[\"compliment_writer\"]+x[\"compliment_photos\"]\n",
    "                                         )))\n",
    "\n",
    "# business, select: business_id,(stars,review_count,attributes[RestaurantsPriceRange2])) \n",
    "# try to add attributes[OutdoorSeating,RestaurantsDelivery,RestaurantsGoodForGroups,RestaurantsReservations,RestaurantsTakeOut] later, True/False\n",
    "business = sc.textFile(business_path).map(lambda x: json.loads(x))\n",
    "business = business.map(lambda x: (x[\"business_id\"],(x[\"stars\"],x[\"review_count\"],getPriceRange(x[\"attributes\"],\"RestaurantsPriceRange2\"))))\n",
    "\n",
    "# review_train, (user_id,business_id,stars)\n",
    "review_train = sc.textFile(review_train_path).map(lambda x: json.loads(x))\n",
    "review_train = review_train.map(lambda x: (x[\"user_id\"],x[\"business_id\"],x[\"stars\"]))\n",
    "\n",
    "# photo, select:business_id,label(['food', 'drink', 'outside', 'inside', 'menu'])\n",
    "photo = sc.textFile(photo_path).map(lambda x: json.loads(x))\n",
    "photo = photo.map(lambda x: (x[\"business_id\"],x[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# aggragation\n",
    "# user, select: user_id,(review_count,fans,average_stars,friends,useful,funny,cool))\n",
    "# review_cnt\n",
    "# if uid not in extra dataset, use the average review_cnt in extra dataset \n",
    "uid_review_cnt_whole = user.map(lambda x: x[1][0]).mean()\n",
    "# fans\n",
    "uid_fans_whole = user.map(lambda x: x[1][1]).mean()\n",
    "# avg_rate\n",
    "uid_avg_rate_whole = user.map(lambda x: x[1][2]).mean()\n",
    "# friends\n",
    "uid_fri_whole = user.map(lambda x: x[1][3]).mean()\n",
    "# (usrful+funny+cool)\n",
    "uid_social_whole = user.map(lambda x: x[1][4]).mean()\n",
    "# var_rate\n",
    "uid_var_rate = review_train.map(lambda x: (x[0],x[2])).groupByKey().mapValues(lambda x: np.var((list(x))))\n",
    "# uid_info: {user_id:(review_count,fans,average_stars,friends,social,year,elite,compliment,var_rate)}\n",
    "uid_info = user.leftOuterJoin(uid_var_rate).map(lambda x: x).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# business, select: business_id,(stars,review_count,attributes[RestaurantsPriceRange2])) \n",
    "# avg_rate\n",
    "bid_avg_rate_whole = business.map(lambda x: x[1][0]).mean()\n",
    "# var_rate\n",
    "bid_var_rate = review_train.map(lambda x: (x[1],x[2])).groupByKey().mapValues(lambda x: np.var((list(x))))\n",
    "# review_cnt\n",
    "bid_review_cnt_whole = business.map(lambda x: x[1][1]).mean()\n",
    "# price_range\n",
    "bid_price_range_whole = business.map(lambda x: x[1][2]).mean()\n",
    "# photo_cnt\n",
    "bid_photo_cnt = photo.filter(lambda x: x[1]!=\"menu\").map(lambda x: (x[0],1)).reduceByKey(lambda x,y:x+y)\n",
    "# bid_info: {business_id:(stars,review_count,price_range,var_rate,phtot_cnt)}\n",
    "bid_info = business.leftOuterJoin(bid_var_rate).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).\\\n",
    "                    leftOuterJoin(bid_photo_cnt).map(lambda x: (x[0],x[1][0]+(fillInNone(x[1][1],0),))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.DataFrame(train_uid_bid_rate.collect(),columns=[\"user_id\",\"business_id\",\"stars\"])\n",
    "df_train = mergrFeatures(df_train_org,uid_info,bid_info)\n",
    "x_train = df_train.drop([\"user_id\",\"business_id\",\"stars\"],axis=1)\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# standarize\n",
    "for col in x_train.columns:\n",
    "    x_train[col] = (x_train[col]-x_train[col].mean())/x_train[col].std()\n",
    "y_train = df_train[\"stars\"]\n",
    "\n",
    "# read test data and train and get basic info\n",
    "test_data = sc.textFile(test_path)\n",
    "test_head = test_data.first()\n",
    "test_data = test_data.filter(lambda x: x!=test_head) #exclude the first line of name\n",
    "uid_bid_to_pred = test_data.map(lambda x: x.split(\",\")).map(lambda x: (x[0],x[1])).collect()\n",
    "df_test_org = pd.DataFrame(uid_bid_to_pred,columns=[\"user_id\",\"business_id\"])\n",
    "df_test = mergrFeatures(df_test_org,uid_info,bid_info)\n",
    "x_test = df_test.drop([\"user_id\",\"business_id\"],axis=1)\n",
    "# x_test = scaler.transform(x_test)\n",
    "for col in x_test.columns:\n",
    "    x_test[col] = (x_test[col]-x_test[col].mean())/x_test[col].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select parameters\n",
    "# xgb_model = xgb.XGBRegressor()\n",
    "# param_grid = {\"alpha\":[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "#             #   \"max_depth\":range(3,10,2),\n",
    "#               \"learning_rate\":[0.05,0.01,0.1]}\n",
    "# grid_search = GridSearchCV(xgb_model,param_grid,cv=5)\n",
    "# grid_search = grid_search.fit(x_train,y_train)\n",
    "# alpha = grid_search.best_params_[\"alpha\"]\n",
    "# learning_rate = grid_search.best_params_[\"learning_rate\"]\n",
    "# print(alpha,learning_rate)\n",
    "\n",
    "# tmp = 10\n",
    "# truth = pd.read_csv(test_path)\n",
    "# for alpha in [0,0.1,0.2,0.3,0.4,0.5,0.6]:\n",
    "#     for lr in [0.05]:\n",
    "#         xgb_model = xgb.XGBRegressor(alpha=alpha,learning_rate=lr,random_state=0)\n",
    "#         xgb_model.fit(x_train,y_train)\n",
    "#         y_pred = xgb_model.predict(x_test)\n",
    "#         rmse = mean_squared_error(truth[\"stars\"],y_pred,squared=False)\n",
    "#         # if rmse<tmp:\n",
    "#         #     tmp = rmse\n",
    "#         print((alpha,lr),rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892251253222397\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xgb_model = xgb.XGBRegressor(alpha=0.5,learning_rate=0.05,colsample_bytree=0.4,max_depth=7,n_estimators=110,subsample=0.6,random_state=0)\n",
    "xgb_model.fit(x_train.drop([\"bsn_price_range\"],axis=1),y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = xgb_model.predict(x_test.drop([\"bsn_price_range\"],axis=1))\n",
    "res_model = pd.DataFrame({\"user_id\":[x[0] for x in uid_bid_to_pred],\"business_id\":[x[1] for x in uid_bid_to_pred],\"prediction\": y_pred})\n",
    "truth = pd.read_csv(test_path)\n",
    "merged_model = truth.merge(res_model,on=[\"user_id\", \"business_id\"])\n",
    "# print(mean_squared_error(merged_model[\"stars\"],res_model[\"prediction\"],squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0.99105265104017\n",
      "0.8105263157894738 0.9908258228097481\n",
      "0.8210526315789474 0.9906134720065277\n",
      "0.8315789473684211 0.9904156079426405\n",
      "0.8421052631578948 0.990232239302125\n",
      "0.8526315789473684 0.99006337413903\n",
      "0.8631578947368421 0.9899090198756494\n",
      "0.8736842105263158 0.9897691833008976\n",
      "0.8842105263157896 0.9896438705688179\n",
      "0.8947368421052632 0.9895330871972323\n",
      "0.9052631578947369 0.9894368380665277\n",
      "0.9157894736842105 0.9893551274185824\n",
      "0.9263157894736842 0.9892879588558325\n",
      "0.9368421052631579 0.989235335340478\n",
      "0.9473684210526316 0.98919725919383\n",
      "0.9578947368421052 0.9891737320957988\n",
      "0.968421052631579 0.9891647550845235\n",
      "0.9789473684210527 0.9891703285561434\n",
      "0.9894736842105263 0.9891904522647095\n",
      "1.0 0.9892251253222397\n"
     ]
    }
   ],
   "source": [
    "# # combine two methods\n",
    "# res_merged = res_model.merge(res_item,on=[\"user_id\",\"business_id\"])\n",
    "# for a in np.linspace(0.8,1,20):\n",
    "#     merged = []\n",
    "#     for i in range(res_merged.shape[0]):\n",
    "#         merged.append(res_merged[\"prediction_x\"][i]*a+res_merged[\"prediction_y\"][i]*(1-a))\n",
    "#     res_merged[\"final_res\"] = merged\n",
    "#     print(a,mean_squared_error(truth[\"stars\"],res_merged[\"final_res\"],squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0490843578388231\n",
      "0.9892251253222397\n",
      "0.9891646635107353\n"
     ]
    }
   ],
   "source": [
    "# combine two methods\n",
    "res_merged = res_model.merge(res_item,on=[\"user_id\",\"business_id\"])\n",
    "merged = []\n",
    "a = 0.97\n",
    "for i in range(res_merged.shape[0]):\n",
    "    merged.append(res_merged[\"prediction_x\"][i]*a+res_merged[\"prediction_y\"][i]*(1-a))\n",
    "res_merged[\"final_res\"] = merged\n",
    "\n",
    "# calculate RMSE < 1.00\n",
    "# merged = truth.merge(res_merged,on=[\"user_id\", \"business_id\"])\n",
    "# print(mean_squared_error(merged_item[\"stars\"],merged_item[\"prediction\"],squared=False))\n",
    "# print(mean_squared_error(merged_model[\"stars\"],merged_model[\"prediction\"],squared=False))\n",
    "# print(mean_squared_error(truth[\"stars\"],merged[\"final_res\"],squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 149.84315705299377\n"
     ]
    }
   ],
   "source": [
    "# less than 100 second\n",
    "e_time = time.time()\n",
    "duration = e_time-s_time\n",
    "print(\"Duration:\",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv, header: user_id, business_id, prediction\n",
    "output= res_merged[[\"user_id\",\"business_id\",\"final_res\"]]\n",
    "output.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export PYSPARK_PYTHON=python3.6                                                                                  \n",
    "#export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  \n",
    "#/opt/spark/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --executor-memory 4G --driver-memory 4G task2_2.py\n",
    "# \"../resource/asnlib/publicdata/\"\n",
    "# \"../resource/asnlib/publicdata/yelp_val.csv\"\n",
    "# \"./task2_2.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "db928fd0c57d8c7a39883c08009f12c1243d97ab72bdd745024349e3e8cdaefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
